{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import gdown\ntrain_file_id = '1CX_H9PXRdCEhRx14ifDObqkhuqOXCWZQ'\ngdown.download(f'https://drive.google.com/uc?export=download&id={train_file_id}', 'train_punctuation_data.jsonl', quiet=False)\ndev_file_id = '1XnJmiji-NCE-goVCxCKfguUnt6mMRoYU'\ngdown.download(f'https://drive.google.com/uc?export=download&id={dev_file_id}', 'dev_punctuation_data.jsonl', quiet=False)\ntest_file_id = '1hqfZq0vKoBqWcUur2on0gqOqHHKOe_Jb'\ngdown.download(f'https://drive.google.com/uc?export=download&id={test_file_id}', 'test_punctuation_data.jsonl', quiet=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T11:51:06.064340Z","iopub.execute_input":"2025-05-04T11:51:06.064577Z","iopub.status.idle":"2025-05-04T11:51:16.627530Z","shell.execute_reply.started":"2025-05-04T11:51:06.064554Z","shell.execute_reply":"2025-05-04T11:51:16.626908Z"}},"outputs":[{"name":"stderr","text":"Downloading...\nFrom: https://drive.google.com/uc?export=download&id=1CX_H9PXRdCEhRx14ifDObqkhuqOXCWZQ\nTo: /kaggle/working/train_punctuation_data.jsonl\n100%|██████████| 3.39M/3.39M [00:00<00:00, 66.2MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?export=download&id=1XnJmiji-NCE-goVCxCKfguUnt6mMRoYU\nTo: /kaggle/working/dev_punctuation_data.jsonl\n100%|██████████| 463k/463k [00:00<00:00, 121MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?export=download&id=1hqfZq0vKoBqWcUur2on0gqOqHHKOe_Jb\nTo: /kaggle/working/test_punctuation_data.jsonl\n100%|██████████| 800k/800k [00:00<00:00, 90.0MB/s]\n","output_type":"stream"},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"'test_punctuation_data.jsonl'"},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"import json\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import MBart50TokenizerFast, MBartModel\nfrom torch import nn, optim\nfrom sklearn.metrics import accuracy_score\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\n\n\ndef load_tokens_and_labels(filepath):\n    tokens, labels = [], []\n    with open(filepath, 'r') as f:\n        for line in f:\n            item = json.loads(line)\n            tokens.append(item['tokens_per_sentence'])\n            label = item['labels']\n            label.insert(0, -99)\n            label.append(-99)\n            label = [l - 1 for l in label]\n            labels.append(label)\n    return tokens, labels\n\n\n# Load the data\ntrain_tokens, train_labels = load_tokens_and_labels(\"train_punctuation_data.jsonl\")\ndev_tokens, dev_labels = load_tokens_and_labels(\"dev_punctuation_data.jsonl\")\ntest_tokens, test_labels = load_tokens_and_labels(\"test_punctuation_data.jsonl\")\n\n\n# Dataset class\ntokenizer = MBart50TokenizerFast.from_pretrained('facebook/mbart-large-50')\n\nclass TokenClassificationDataset(Dataset):\n    def __init__(self, tokens, labels):\n        self.tokens = tokens\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.tokens)\n\n    def __getitem__(self, idx):\n        encoding = tokenizer(self.tokens[idx],\n                             is_split_into_words=True,\n                             padding=False,\n                             truncation=True,\n                             return_tensors=None)\n        input_ids = torch.tensor(encoding['input_ids'])\n        attention_mask = torch.tensor(encoding['attention_mask'])\n        return {'input_ids': input_ids, 'attention_mask': attention_mask}, self.labels[idx]\n\n\n# Collate Function\ndef collate_fn(batch):\n    filtered_batch = [item for item in batch if item[0]['input_ids'].size(0) <= 60]\n    discarded = len(batch) - len(filtered_batch)\n    if len(filtered_batch) == 0:\n        return None, None, discarded\n\n    max_length = max(item[0]['input_ids'].size(0) for item in filtered_batch)\n\n    input_ids = torch.stack([\n        torch.cat([item[0]['input_ids'], torch.zeros(max_length - item[0]['input_ids'].size(0), dtype=torch.long)])\n        for item in filtered_batch\n    ])\n    attention_mask = torch.stack([\n        torch.cat([item[0]['attention_mask'], torch.zeros(max_length - item[0]['attention_mask'].size(0), dtype=torch.long)])\n        for item in filtered_batch\n    ])\n    labels = torch.stack([\n        torch.cat([torch.tensor(item[1], dtype=torch.long), torch.full((max_length - len(item[1]),), -100, dtype=torch.long)])\n        for item in filtered_batch\n    ])\n\n    return {'input_ids': input_ids.to(device), 'attention_mask': attention_mask.to(device)}, labels.to(device), discarded\n\n\n\n# Model class\nclass PunctuationModel(nn.Module):\n    def __init__(self, model_name, num_labels):\n        super(PunctuationModel, self).__init__()\n        self.mbart = MBartModel.from_pretrained(model_name)\n        self.classifier = nn.Linear(self.mbart.config.d_model, num_labels)\n\n    def forward(self, input_ids, attention_mask=None):\n        outputs = self.mbart(input_ids=input_ids, attention_mask=attention_mask)\n        hidden_states = outputs.last_hidden_state\n        logits = self.classifier(hidden_states)\n        return logits\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Datasets and loaders\ntrain_dataset = TokenClassificationDataset(train_tokens, train_labels)\ndev_dataset = TokenClassificationDataset(dev_tokens, dev_labels)\ntest_dataset = TokenClassificationDataset(test_tokens, test_labels)\n\ntrain_loader = DataLoader(train_dataset, batch_size=8, collate_fn=collate_fn, shuffle=True)\ndev_loader = DataLoader(dev_dataset, batch_size=8, collate_fn=collate_fn, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=8, collate_fn=collate_fn, shuffle=False)\n\n# Training and Evaluation\nmodel = PunctuationModel('facebook/mbart-large-50', num_labels=6).to(device)\noptimizer = optim.Adam(model.parameters(), lr=1e-5)\nloss_fn = nn.CrossEntropyLoss()\n\n\ndef evaluate(dataloader):\n    model.eval()\n    all_preds = []\n    all_labels = []\n    total_discarded = 0\n    running_loss = 0\n\n    with torch.no_grad():\n        for batch in dataloader:\n            if batch[0] is None:\n                total_discarded += batch[2]\n                continue\n\n            input_ids = batch[0]['input_ids']\n            attention_mask = batch[0]['attention_mask']\n            labels = batch[1]\n\n            logits = model(input_ids, attention_mask=attention_mask)\n\n            logits = logits.view(-1, logits.shape[-1])\n            labels = labels.view(-1)\n\n            loss = loss_fn(logits, labels)\n            running_loss += loss.item()\n\n            preds = torch.argmax(logits, dim=1)\n            mask = labels != -100\n            all_preds.extend(preds[mask].tolist())\n            all_labels.extend(labels[mask].tolist())\n\n    avg_loss = running_loss / len(dataloader)\n    acc = accuracy_score(all_labels, all_preds)\n    return avg_loss, acc, total_discarded\n\n\n\ndef train_one_epoch(dataloader):\n    model.train()\n    running_loss = 0\n    all_preds = []\n    all_labels = []\n    total_discarded = 0\n\n    for batch in dataloader:\n        if batch[0] is None:\n            total_discarded += batch[2]\n            continue\n\n        input_ids = batch[0]['input_ids']\n        attention_mask = batch[0]['attention_mask']\n        labels = batch[1]\n        discarded = batch[2]\n        total_discarded += discarded\n\n        optimizer.zero_grad()\n        logits = model(input_ids, attention_mask=attention_mask)\n\n        logits = logits.view(-1, logits.shape[-1])\n        labels = labels.view(-1)\n\n        loss = loss_fn(logits, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n\n        preds = torch.argmax(logits, dim=1)\n        mask = labels != -100\n        all_preds.extend(preds[mask].tolist())\n        all_labels.extend(labels[mask].tolist())\n\n    avg_loss = running_loss / len(dataloader)\n    acc = accuracy_score(all_labels, all_preds)\n    return avg_loss, acc, total_discarded\n\n\n\n# Early stopping training loop\ndef train_for_epochs(train_loader, val_loader, epochs=10, patience=2):\n    best_val_acc = 0\n    patience_counter = 0\n\n    for epoch in range(epochs):\n        print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n        train_loss, train_acc, train_discarded = train_one_epoch(train_loader)\n        val_loss, val_acc, val_discarded = evaluate(val_loader)\n\n        print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Discarded: {train_discarded}\")\n        print(f\"Val   Loss: {val_loss:.4f} | Val   Acc: {val_acc:.4f} | Discarded: {val_discarded}\")\n        print('-' * 60)\n\n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            patience_counter = 0\n            torch.save(model.state_dict(), 'best_mbart_punctuation_model.pt')\n        else:\n            patience_counter += 1\n            if patience_counter >= patience:\n                print(\"Early stopping triggered.\")\n                break\n\n    print(f\"Final Train Accuracy: {train_acc:.4f} | Final Dev Accuracy: {val_acc:.4f}\")\n\n\n#train_for_epochs(train_loader, dev_loader, epochs=10, patience=2)\n\n# Load best model and evaluate on test\n# model.load_state_dict(torch.load('best_mbart_punctuation_model.pt'))\n# test_acc = evaluate(test_loader)\n# print(f\"Test Accuracy: {test_acc:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T11:51:19.127873Z","iopub.execute_input":"2025-05-04T11:51:19.128431Z","iopub.status.idle":"2025-05-04T11:52:07.682953Z","shell.execute_reply.started":"2025-05-04T11:51:19.128406Z","shell.execute_reply":"2025-05-04T11:52:07.681933Z"}},"outputs":[{"name":"stderr","text":"2025-05-04 11:51:34.722054: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1746359495.025176      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1746359495.100682      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/531 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0b12d4e810d4d02a20cc950529ca86e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c0bd5645a8b4843ae48cfd0d4ccba4b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/649 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"23965029adbe405db8c895cc4fab3dea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.42k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a30ab45a61d42728fb41fcee186cb06"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/2.44G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cadba8186cbd40c598a22ea82f58b6e4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.44G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a0290629f6046288c5642214b4cd616"}},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"import gdown\nmodel_id = '1uU3tcfKQGomyldf5KmMczFtpzz4IYXb3'\ngdown.download(f'https://drive.google.com/uc?export=download&id={model_id}', 'best_test_punctuation_model.pt', quiet=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T11:52:19.019650Z","iopub.execute_input":"2025-05-04T11:52:19.020199Z","iopub.status.idle":"2025-05-04T11:52:52.651618Z","shell.execute_reply.started":"2025-05-04T11:52:19.020174Z","shell.execute_reply":"2025-05-04T11:52:52.650686Z"}},"outputs":[{"name":"stderr","text":"Downloading...\nFrom (original): https://drive.google.com/uc?export=download&id=1uU3tcfKQGomyldf5KmMczFtpzz4IYXb3\nFrom (redirected): https://drive.google.com/uc?export=download&id=1uU3tcfKQGomyldf5KmMczFtpzz4IYXb3&confirm=t&uuid=ea9a0b54-d0a4-4756-9c03-d2368a3c2a4b\nTo: /kaggle/working/best_test_punctuation_model.pt\n100%|██████████| 2.44G/2.44G [00:22<00:00, 108MB/s] \n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"'best_test_punctuation_model.pt'"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"state_dict = torch.load('best_test_punctuation_model.pt', weights_only=True)\nmodel.load_state_dict(state_dict)\navg_loss, acc, total_discarded = evaluate(test_loader)\nprint(f\"Test Accuracy: {acc:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T11:53:05.040477Z","iopub.execute_input":"2025-05-04T11:53:05.040999Z","iopub.status.idle":"2025-05-04T11:53:14.216063Z","shell.execute_reply.started":"2025-05-04T11:53:05.040976Z","shell.execute_reply":"2025-05-04T11:53:14.215109Z"}},"outputs":[{"name":"stdout","text":"Test Accuracy: 0.8942\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import gdown\nfile_id = '1FR-MyJRxZH62grW7DihFqbgA8HG0Wnne'\ngdown.download(f'https://drive.google.com/uc?export=download&id={file_id}', 'dev_punctuation_data.csv', quiet=False)\n\nfile_id = '1e0JBRmo98zKK8Lbfof9QFZ7IlQiYtDC2'\ngdown.download(f'https://drive.google.com/uc?export=download&id={file_id}', 'test_punctuation_data.csv', quiet=False)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T11:53:19.572533Z","iopub.execute_input":"2025-05-04T11:53:19.573212Z","iopub.status.idle":"2025-05-04T11:53:26.434895Z","shell.execute_reply.started":"2025-05-04T11:53:19.573186Z","shell.execute_reply":"2025-05-04T11:53:26.434359Z"}},"outputs":[{"name":"stderr","text":"Downloading...\nFrom: https://drive.google.com/uc?export=download&id=1FR-MyJRxZH62grW7DihFqbgA8HG0Wnne\nTo: /kaggle/working/dev_punctuation_data.csv\n100%|██████████| 227k/227k [00:00<00:00, 84.4MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?export=download&id=1e0JBRmo98zKK8Lbfof9QFZ7IlQiYtDC2\nTo: /kaggle/working/test_punctuation_data.csv\n100%|██████████| 356k/356k [00:00<00:00, 115MB/s]\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"'test_punctuation_data.csv'"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"import pandas as pd\ndf = pd.read_csv('dev_punctuation_data.csv')\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T11:53:38.821352Z","iopub.execute_input":"2025-05-04T11:53:38.821988Z","iopub.status.idle":"2025-05-04T11:53:38.854221Z","shell.execute_reply.started":"2025-05-04T11:53:38.821963Z","shell.execute_reply":"2025-05-04T11:53:38.853629Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n  has_large_values = (abs_vals > 1e6).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"     id                  filename  \\\n0  1599   9909934339673808373.wav   \n1  1608   4582604850545677686.wav   \n2  1533  13033938513680724611.wav   \n3  1585    369662424302423610.wav   \n4  1594  11667832843141994163.wav   \n\n                                           actual_en  \\\n0  The original population hasn't changed at all,...   \n1  I lost my sister and her friend, and on my way...   \n2  Many common formats (APS family of formats, fo...   \n3  Over time, as the new population begins to ada...   \n4  Some animals, such as elephants and giraffes, ...   \n\n                                         asr_decoded  \\\n0  The original population hasn't changed at all....   \n1  I lost my sister and her friend, and on my way...   \n2  Many common formats, APS family of formats for...   \n3  Over time, as the population begins to adapt t...   \n4  Some animals, such as elephants and giraffes, ...   \n\n                                           actual_te  baseline_mt_translated  \n0  అసలు జనాభా ఏ మాత్రం మారలేదు, వారికి మునుపటి లా...                     NaN  \n1  నేను నా సోదరిని ఇంకా ఆమె స్నేహితుడిని కోల్పోయా...                     NaN  \n2  అనేక  సాధారణ ఫార్మాట్‌లు (ఉదాహరణకు ఫార్మాట్‌ల ...                     NaN  \n3  కాలక్రమేణా, కొత్త జనాభా వారి కొత్త వాతావరణానిక...                     NaN  \n4  కొన్ని జంతువులు, ఏనుగులు మరియు జిరాఫీలు వంటి జ...                     NaN  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>filename</th>\n      <th>actual_en</th>\n      <th>asr_decoded</th>\n      <th>actual_te</th>\n      <th>baseline_mt_translated</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1599</td>\n      <td>9909934339673808373.wav</td>\n      <td>The original population hasn't changed at all,...</td>\n      <td>The original population hasn't changed at all....</td>\n      <td>అసలు జనాభా ఏ మాత్రం మారలేదు, వారికి మునుపటి లా...</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1608</td>\n      <td>4582604850545677686.wav</td>\n      <td>I lost my sister and her friend, and on my way...</td>\n      <td>I lost my sister and her friend, and on my way...</td>\n      <td>నేను నా సోదరిని ఇంకా ఆమె స్నేహితుడిని కోల్పోయా...</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1533</td>\n      <td>13033938513680724611.wav</td>\n      <td>Many common formats (APS family of formats, fo...</td>\n      <td>Many common formats, APS family of formats for...</td>\n      <td>అనేక  సాధారణ ఫార్మాట్‌లు (ఉదాహరణకు ఫార్మాట్‌ల ...</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1585</td>\n      <td>369662424302423610.wav</td>\n      <td>Over time, as the new population begins to ada...</td>\n      <td>Over time, as the population begins to adapt t...</td>\n      <td>కాలక్రమేణా, కొత్త జనాభా వారి కొత్త వాతావరణానిక...</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1594</td>\n      <td>11667832843141994163.wav</td>\n      <td>Some animals, such as elephants and giraffes, ...</td>\n      <td>Some animals, such as elephants and giraffes, ...</td>\n      <td>కొన్ని జంతువులు, ఏనుగులు మరియు జిరాఫీలు వంటి జ...</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":7},{"cell_type":"raw","source":"","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nimport pandas as pd\nfrom tqdm import tqdm\nimport json  # Still needed if using tokenizer from transformers\n# Ensure you have these already defined:\n# - tokenizer\n# - model\n# - device\n\n# Path to your CSV file\ninput_csv = 'dev_punctuation_data.csv'\noutput_dir = 'dev_asr_text_punct_features/'  # Folder to save features\n\nos.makedirs(output_dir, exist_ok=True)\n\n# Load CSV\ndf = pd.read_csv(input_csv, encoding='utf-8',on_bad_lines='skip')\n\nfor _, row in tqdm(df.iterrows(), total=len(df), desc=\"Extracting features\"):\n    uid = str(row['filename'])\n    original_text = str(row['asr_decoded'])\n    # print(original_text)\n    # print(uid)\n\n    # Tokenize\n    encoded = tokenizer(original_text, return_tensors='pt', padding=True, truncation=True)\n    input_ids = encoded['input_ids'].to(device)\n    attention_mask = encoded['attention_mask'].to(device)\n    \n    with torch.no_grad():\n        outputs = model.mbart(input_ids=input_ids, attention_mask=attention_mask)\n        last_hidden_state = outputs.last_hidden_state.squeeze(0).cpu()  # [seq_len, hidden_size]\n\n    # Save to a file named by ID\n    torch.save(last_hidden_state, f\"{output_dir}/{uid}.pt\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T11:54:55.360003Z","iopub.execute_input":"2025-05-04T11:54:55.360310Z","iopub.status.idle":"2025-05-04T11:55:02.541443Z","shell.execute_reply.started":"2025-05-04T11:54:55.360264Z","shell.execute_reply":"2025-05-04T11:55:02.540670Z"}},"outputs":[{"name":"stderr","text":"Extracting features: 100%|██████████| 365/365 [00:07<00:00, 50.92it/s]\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import os\nimport torch\nimport pandas as pd\nfrom tqdm import tqdm\nimport json  # Still needed if using tokenizer from transformers\n# Ensure you have these already defined:\n# - tokenizer\n# - model\n# - device\n\n# Path to your CSV file\ninput_csv = 'test_punctuation_data.csv'\noutput_dir = 'test_asr_text_punct_features/'  # Folder to save features\n\nos.makedirs(output_dir, exist_ok=True)\n\n# Load CSV\ndf = pd.read_csv(input_csv, encoding='utf-8',on_bad_lines='skip')\n\nfor _, row in tqdm(df.iterrows(), total=len(df), desc=\"Extracting features\"):\n    uid = str(row['filename'])\n    original_text = str(row['asr_decoded'])\n    # print(original_text)\n    # print(uid)\n\n    # Tokenize\n    encoded = tokenizer(original_text, return_tensors='pt', padding=True, truncation=True)\n    input_ids = encoded['input_ids'].to(device)\n    attention_mask = encoded['attention_mask'].to(device)\n    \n    with torch.no_grad():\n        outputs = model.mbart(input_ids=input_ids, attention_mask=attention_mask)\n        last_hidden_state = outputs.last_hidden_state.squeeze(0).cpu()  # [seq_len, hidden_size]\n\n    # Save to a file named by ID\n    torch.save(last_hidden_state, f\"{output_dir}/{uid}.pt\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T11:55:32.902738Z","iopub.execute_input":"2025-05-04T11:55:32.903367Z","iopub.status.idle":"2025-05-04T11:55:43.509208Z","shell.execute_reply.started":"2025-05-04T11:55:32.903342Z","shell.execute_reply":"2025-05-04T11:55:43.508533Z"}},"outputs":[{"name":"stderr","text":"Extracting features: 100%|██████████| 554/554 [00:10<00:00, 52.30it/s]\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"import shutil\n\nshutil.make_archive(\"dev_asr_text_punct_features\", 'zip', \"dev_asr_text_punct_features/\")\n\nfrom IPython.display import FileLink\nFileLink('dev_asr_text_punct_features.zip')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T11:57:09.236564Z","iopub.execute_input":"2025-05-04T11:57:09.236840Z","iopub.status.idle":"2025-05-04T11:57:11.331271Z","shell.execute_reply.started":"2025-05-04T11:57:09.236820Z","shell.execute_reply":"2025-05-04T11:57:11.330602Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/dev_asr_text_punct_features.zip","text/html":"<a href='dev_asr_text_punct_features.zip' target='_blank'>dev_asr_text_punct_features.zip</a><br>"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"import shutil\n\nshutil.make_archive(\"test_asr_text_punct_features\", 'zip', \"test_asr_text_punct_features/\")\n\nfrom IPython.display import FileLink\nFileLink('test_asr_text_punct_features.zip')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T11:57:28.329600Z","iopub.execute_input":"2025-05-04T11:57:28.329867Z","iopub.status.idle":"2025-05-04T11:57:31.622914Z","shell.execute_reply.started":"2025-05-04T11:57:28.329847Z","shell.execute_reply":"2025-05-04T11:57:31.622345Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/test_asr_text_punct_features.zip","text/html":"<a href='test_asr_text_punct_features.zip' target='_blank'>test_asr_text_punct_features.zip</a><br>"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
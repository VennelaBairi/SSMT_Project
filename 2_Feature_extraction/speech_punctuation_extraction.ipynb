{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1t2IpS8wAVNSkvNrMj8sNf5kHBa8ky7Td","authorship_tag":"ABX9TyMgBJaaqaMp2sVlKTc7c6IW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["import scipy.io\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from tqdm import tqdm\n","import copy\n","import numpy as np"],"metadata":{"id":"ewXyrwIjtSCt","executionInfo":{"status":"ok","timestamp":1746207721400,"user_tz":-330,"elapsed":6394,"user":{"displayName":"Sai Harshitha Aluru","userId":"02296733820851712734"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","execution_count":17,"metadata":{"id":"ehqWOVCjmtxK","executionInfo":{"status":"ok","timestamp":1746208737886,"user_tz":-330,"elapsed":7990,"user":{"displayName":"Sai Harshitha Aluru","userId":"02296733820851712734"}}},"outputs":[],"source":["\n","# Load data\n","train_data = scipy.io.loadmat('/content/drive/MyDrive/SSMT/train_speech_subtoken_punc_features.mat')\n","dev_data = scipy.io.loadmat('/content/drive/MyDrive/SSMT/dev_speech_subtoken_punc_features.mat')\n","test_data = scipy.io.loadmat('/content/drive/MyDrive/SSMT/test_speech_subtoken_punc_features.mat')\n","\n","# Extract features and labels\n","filenames_train, X_train, y_train = train_data['filenames'], train_data['features'], train_data['labels']\n","filenames_dev, X_dev, y_dev = dev_data['filenames'], dev_data['features'], dev_data['labels']\n","filenames_test, X_test, y_test = test_data['filenames'], test_data['features'], test_data['labels']"]},{"cell_type":"code","source":["print(X_train.T[0][0].shape)\n","print(y_train.shape)\n","print(X_dev.shape)\n","print(y_dev.shape)\n","print(filenames_train.shape)\n","print(filenames_dev.shape)\n","print(X_test.shape)\n","print(y_test.shape)\n","print(filenames_test.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RgCx4YhgF2_b","executionInfo":{"status":"ok","timestamp":1746208737944,"user_tz":-330,"elapsed":57,"user":{"displayName":"Sai Harshitha Aluru","userId":"02296733820851712734"}},"outputId":"68963f44-7036-46a3-fff4-5ccd5009b0fd"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["(35, 768)\n","(1, 1545)\n","(1, 255)\n","(1, 255)\n","(1545,)\n","(255,)\n","(1, 399)\n","(1, 399)\n","(399,)\n"]}]},{"cell_type":"code","source":["print(X_train.T[0][0].shape)\n","print(y_train.shape)\n","print(X_dev.shape)\n","print(y_dev.shape)\n","print(filenames_train.shape)\n","print(filenames_dev.shape)\n","print(X_test.shape)\n","print(y_test.shape)\n","print(filenames_test.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0kIB-Q4tn03Q","executionInfo":{"status":"ok","timestamp":1746208737948,"user_tz":-330,"elapsed":3,"user":{"displayName":"Sai Harshitha Aluru","userId":"02296733820851712734"}},"outputId":"487b9737-b128-4936-e774-9c16ef6361c4"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["(35, 768)\n","(1, 1545)\n","(1, 255)\n","(1, 255)\n","(1545,)\n","(255,)\n","(1, 399)\n","(1, 399)\n","(399,)\n"]}]},{"cell_type":"code","source":["# Shift all labels from 1–6 to 0–5\n","for i in range(y_train.shape[1]):\n","    y_train[0][i] = y_train[0][i] - 1\n","\n","for i in range(y_dev.shape[1]):\n","    y_dev[0][i] = y_dev[0][i] - 1\n","\n","for i in range(y_test.shape[1]):\n","    y_test[0][i] = y_test[0][i] - 1"],"metadata":{"id":"k3KnWylnqKu8","executionInfo":{"status":"ok","timestamp":1746208744070,"user_tz":-330,"elapsed":2,"user":{"displayName":"Sai Harshitha Aluru","userId":"02296733820851712734"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["print(y_train[0][0], y_dev[0][0], y_test[0][0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p-7a238RqVmY","executionInfo":{"status":"ok","timestamp":1746208744271,"user_tz":-330,"elapsed":24,"user":{"displayName":"Sai Harshitha Aluru","userId":"02296733820851712734"}},"outputId":"724d34ea-6337-4628-dfb5-690a5da1b43f"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["[[5 0 0 0 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 1 1]] [[5 5 5 5 5 5 5 5 0 0 0 5 5 5 5 5 0 0 5 5 5 1 1 1 5 5 5 5 5 5 1 1 1 1]] [[0 0 5 5 5 5 5 5 5 5 5 5 5 5 5 5 4 4 4 4 5 1 1 1]]\n"]}]},{"cell_type":"code","source":["print(y_dev.flatten().shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JFXiotjNAI_8","executionInfo":{"status":"ok","timestamp":1746208746169,"user_tz":-330,"elapsed":18,"user":{"displayName":"Sai Harshitha Aluru","userId":"02296733820851712734"}},"outputId":"dac3f4c5-4972-4b68-a34e-8965cc82868a"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["(255,)\n"]}]},{"cell_type":"code","source":["l=[]\n","for i in y_dev.flatten():\n","  for j in i.flatten():\n","    if j not in l:\n","      l.append(j)\n","print(l)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ygqaTQeH__C0","executionInfo":{"status":"ok","timestamp":1746208746511,"user_tz":-330,"elapsed":24,"user":{"displayName":"Sai Harshitha Aluru","userId":"02296733820851712734"}},"outputId":"38aa67b0-d0e8-456e-e172-f07cf0b798d5"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["[np.int64(5), np.int64(0), np.int64(1), np.int64(3), np.int64(4)]\n"]}]},{"cell_type":"code","source":["# class PunctuationDataset(torch.utils.data.Dataset):\n","#     def __init__(self, X, Y):\n","#         self.X = [torch.tensor(X.T[i][0], dtype=torch.float32) for i in range(X.shape[1])]\n","#         self.Y = [torch.tensor(Y.T[i][0].flatten(), dtype=torch.long) for i in range(Y.shape[1])]\n","\n","#         # Optional: sanity check\n","#         for x, y in zip(self.X, self.Y):\n","#             assert len(x) == len(y), f\"Mismatched lengths: {x.shape[0]} vs {y.shape[0]}\"\n","\n","#     def __len__(self):\n","#         return len(self.X)\n","\n","#     def __getitem__(self, idx):\n","#         return self.X[idx], self.Y[idx]\n","class PunctuationDataset(torch.utils.data.Dataset):\n","    def __init__(self, X, Y, filenames):\n","        self.X = [torch.tensor(X.T[i][0], dtype=torch.float32) for i in range(X.shape[1])]\n","        self.Y = [torch.tensor(Y.T[i][0].flatten(), dtype=torch.long) for i in range(Y.shape[1])]\n","        self.filenames = filenames\n","\n","        assert len(self.X) == len(self.filenames), \"Mismatch between data and filenames\"\n","\n","        for x, y in zip(self.X, self.Y):\n","            assert len(x) == len(y), f\"Mismatched lengths: {x.shape[0]} vs {y.shape[0]}\"\n","\n","    def __len__(self):\n","        return len(self.X)\n","\n","    def __getitem__(self, idx):\n","        return self.X[idx], self.Y[idx], self.filenames[idx]\n"],"metadata":{"id":"PApvUlyEndhA","executionInfo":{"status":"ok","timestamp":1746208748361,"user_tz":-330,"elapsed":8,"user":{"displayName":"Sai Harshitha Aluru","userId":"02296733820851712734"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["# def collate_fn(batch):\n","#     xs, ys = zip(*batch)\n","#     lengths = [len(x) for x in xs]\n","\n","#     xs_padded = nn.utils.rnn.pad_sequence(xs, batch_first=True)  # [batch, max_len, feat_dim]\n","#     ys_padded = nn.utils.rnn.pad_sequence(ys, batch_first=True, padding_value=-100)  # ignore_index\n","\n","#     return xs_padded, ys_padded, lengths\n","def collate_fn(batch):\n","    xs, ys, filenames = zip(*batch)\n","    lengths = [len(x) for x in xs]\n","\n","    xs_padded = nn.utils.rnn.pad_sequence(xs, batch_first=True)\n","    ys_padded = nn.utils.rnn.pad_sequence(ys, batch_first=True, padding_value=-100)\n","\n","    return xs_padded, ys_padded, lengths, filenames\n"],"metadata":{"id":"i6jaZcxOo_2Y","executionInfo":{"status":"ok","timestamp":1746208749895,"user_tz":-330,"elapsed":395,"user":{"displayName":"Sai Harshitha Aluru","userId":"02296733820851712734"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["# Assuming you have a list of filenames (e.g., filenames_train for the train data)\n","train_dataset = PunctuationDataset(X_train, y_train, filenames_train)  # Pass filenames_train\n","val_dataset = PunctuationDataset(X_dev, y_dev, filenames_dev)  # Pass filenames_dev\n","test_dataset = PunctuationDataset(X_test, y_test, filenames_test)\n","\n","# Create DataLoader\n","train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, collate_fn=collate_fn)\n","val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)\n","test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)"],"metadata":{"id":"cTO3bKkQpBNe","executionInfo":{"status":"ok","timestamp":1746208750799,"user_tz":-330,"elapsed":98,"user":{"displayName":"Sai Harshitha Aluru","userId":"02296733820851712734"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["class ConvLSTMClassifier(nn.Module):\n","    def __init__(self, input_dim, lstm_hidden=512, num_classes=5):\n","        super(ConvLSTMClassifier, self).__init__()\n","        self.conv = nn.Conv1d(in_channels=input_dim, out_channels=128, kernel_size=5, padding=2)\n","\n","        self.lstm = nn.LSTM(input_size=128, hidden_size=lstm_hidden, batch_first=True, bidirectional=True)\n","\n","        self.linear_1024 = nn.Linear(2 * lstm_hidden, 1024)\n","        self.output_layer = nn.Linear(1024, num_classes)\n","\n","    def forward(self, x_list, lengths):\n","        conv_outs = []\n","        for x in x_list:\n","            x = x.permute(1, 0).unsqueeze(0)\n","            x = self.conv(x)\n","            x = x.squeeze(0).permute(1, 0)\n","            conv_outs.append(x)\n","\n","        padded = nn.utils.rnn.pad_sequence(conv_outs, batch_first=True)\n","        packed = nn.utils.rnn.pack_padded_sequence(padded, lengths, batch_first=True, enforce_sorted=False)\n","\n","        lstm_out, _ = self.lstm(packed)\n","        lstm_out, _ = nn.utils.rnn.pad_packed_sequence(lstm_out, batch_first=True)\n","\n","        linear_1024_out = self.linear_1024(lstm_out)\n","        logits = self.output_layer(linear_1024_out)\n","\n","        # Return both the logits and the features from the last LSTM output\n","        return logits, lstm_out\n"],"metadata":{"id":"4NTDo8KupCs4","executionInfo":{"status":"ok","timestamp":1746208751875,"user_tz":-330,"elapsed":24,"user":{"displayName":"Sai Harshitha Aluru","userId":"02296733820851712734"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["def train_model(model, train_loader, val_loader, num_epochs=20, lr=1e-3, patience=5, device='cuda'):\n","    model = model.to(device)\n","    criterion = nn.CrossEntropyLoss(ignore_index=-100)  # ignore padding values\n","    optimizer = optim.Adam(model.parameters(), lr=lr)\n","\n","    skipped_count = 0  # Counter for skipped NaN cases\n","\n","    # Early stopping variables\n","    best_val_loss = float('inf')\n","    patience_counter = 0\n","\n","    for epoch in range(num_epochs):\n","        model.train()\n","        train_loss = 0\n","        correct_train = 0\n","        total_train = 0\n","        train_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\")\n","\n","        for x_batch, y_batch, lengths, filenames_train in train_bar:\n","            x_batch = x_batch.to(device)\n","            y_batch = y_batch.to(device)\n","\n","            optimizer.zero_grad()\n","            # Unpack the output tuple into `features` and `logits`\n","            logits, lstm_out = model(x_batch, lengths)  # logits are the output before the final layer\n","\n","            # Check for NaN or Inf in logits (not features)\n","            if torch.isnan(logits).any() or torch.isinf(logits).any():\n","                print(\"NaN or Inf detected in model output, skipping this batch\")\n","                skipped_count += 1\n","                continue  # Skip this batch\n","\n","            logits = logits.view(-1, logits.shape[-1])  # Flatten for loss calculation\n","            y_batch = y_batch.view(-1)\n","\n","            # Check for NaN or Inf in loss\n","            loss = criterion(logits, y_batch)\n","            if torch.isnan(loss) or torch.isinf(loss):\n","                print(\"NaN or Inf detected in loss, skipping this batch\")\n","                skipped_count += 1\n","                continue  # Skip this batch\n","\n","            loss.backward()\n","            optimizer.step()\n","\n","            train_loss += loss.item()\n","\n","            # Calculate training accuracy\n","            preds = logits.argmax(dim=-1)\n","            mask = y_batch != -100\n","            correct_train += (preds[mask] == y_batch[mask]).sum().item()\n","            total_train += mask.sum().item()\n","\n","            train_bar.set_postfix(loss=loss.item())\n","\n","        avg_train_loss = train_loss / len(train_loader)\n","        train_acc = correct_train / total_train if total_train > 0 else 0\n","\n","        # --- Validation ---\n","        from sklearn.metrics import classification_report\n","\n","        # In the validation loop:\n","        model.eval()\n","        val_loss = 0\n","        correct = 0\n","        total = 0\n","        predictions = []\n","        ground_truth = []\n","        val_bar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\", leave=False)\n","\n","        with torch.no_grad():\n","            for x_batch, y_batch, lengths, filenames_dev in val_bar:\n","                x_batch = x_batch.to(device)\n","                y_batch = y_batch.to(device)\n","\n","                outputs, lstm_out = model(x_batch, lengths)\n","                outputs = outputs.view(-1, outputs.shape[-1])\n","                y_batch = y_batch.view(-1)\n","\n","                # Compute the loss\n","                loss = criterion(outputs, y_batch)\n","                val_loss += loss.item()\n","\n","                # Get the predictions\n","                preds = outputs.argmax(dim=-1)\n","                mask = y_batch != -100\n","                correct += (preds[mask] == y_batch[mask]).sum().item()\n","                total += mask.sum().item()\n","\n","                # Store predictions and ground truth\n","                predictions.extend(preds[mask].cpu().numpy())\n","                ground_truth.extend(y_batch[mask].cpu().numpy())\n","\n","        avg_val_loss = val_loss / len(val_loader)\n","        val_acc = correct / total if total > 0 else 0\n","\n","        # Print classification report for per-class metrics\n","        print(\"\\nClassification Report:\")\n","        print(classification_report(ground_truth, predictions, zero_division=0))\n","\n","        # Print summary of metrics\n","        print(f\"Epoch {epoch+1}/{num_epochs} Summary: Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n","\n","        print(f\"Skipped {skipped_count} batches due to NaN/Inf\")\n","\n","        # Early stopping\n","        if avg_val_loss < best_val_loss:\n","            best_val_loss = avg_val_loss\n","            patience_counter = 0\n","            # Optionally, save the best model\n","            torch.save(model.state_dict(), 'best_model.pth')\n","        else:\n","            patience_counter += 1\n","            if patience_counter >= patience:\n","                print(f\"Early stopping triggered after {patience} epochs without improvement.\")\n","                break\n","\n","        # Gradient Clipping to prevent explosion\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n"],"metadata":{"id":"jCoIZpNMpJYD","executionInfo":{"status":"ok","timestamp":1746208752710,"user_tz":-330,"elapsed":45,"user":{"displayName":"Sai Harshitha Aluru","userId":"02296733820851712734"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["model = ConvLSTMClassifier(input_dim=768, lstm_hidden=512, num_classes=6)\n"],"metadata":{"id":"S5DqFzpJpbPV","executionInfo":{"status":"ok","timestamp":1746208753813,"user_tz":-330,"elapsed":40,"user":{"displayName":"Sai Harshitha Aluru","userId":"02296733820851712734"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["train_model(model, train_loader, val_loader, num_epochs=50, patience=5)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dvwKC2SDpuHT","executionInfo":{"status":"ok","timestamp":1746105120348,"user_tz":-330,"elapsed":235341,"user":{"displayName":"Sai Harshitha Aluru","userId":"02296733820851712734"}},"outputId":"ba9a0581-3014-4a57-d66b-5c3647f23a84"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Epoch 1/50 [Train]:  42%|████▏     | 659/1556 [00:11<00:07, 114.35it/s, loss=0.475]"]},{"output_type":"stream","name":"stdout","text":["NaN or Inf detected in model output, skipping this batch\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 1/50 [Train]: 100%|██████████| 1556/1556 [00:20<00:00, 76.71it/s, loss=0.309]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00       423\n","           1       0.94      0.74      0.83       402\n","           3       0.00      0.00      0.00        12\n","           4       0.00      0.00      0.00         4\n","           5       0.91      1.00      0.95      5516\n","\n","    accuracy                           0.91      6357\n","   macro avg       0.37      0.35      0.36      6357\n","weighted avg       0.85      0.91      0.88      6357\n","\n","Epoch 1/50 Summary: Train Loss: 0.3282 | Val Loss: 0.2908 | Val Acc: 0.9114\n","Skipped 1 batches due to NaN/Inf\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 2/50 [Train]:  24%|██▍       | 377/1556 [00:04<00:12, 92.56it/s, loss=0.973]"]},{"output_type":"stream","name":"stdout","text":["NaN or Inf detected in model output, skipping this batch\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 2/50 [Train]: 100%|██████████| 1556/1556 [00:18<00:00, 86.28it/s, loss=0.642]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00       423\n","           1       0.94      0.73      0.82       402\n","           3       0.00      0.00      0.00        12\n","           4       0.00      0.00      0.00         4\n","           5       0.91      1.00      0.95      5516\n","\n","    accuracy                           0.91      6357\n","   macro avg       0.37      0.35      0.35      6357\n","weighted avg       0.85      0.91      0.88      6357\n","\n","Epoch 2/50 Summary: Train Loss: 0.3021 | Val Loss: 0.2886 | Val Acc: 0.9108\n","Skipped 2 batches due to NaN/Inf\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 3/50 [Train]:  69%|██████▉   | 1073/1556 [00:12<00:05, 93.80it/s, loss=0.182]"]},{"output_type":"stream","name":"stdout","text":["NaN or Inf detected in model output, skipping this batch\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 3/50 [Train]: 100%|██████████| 1556/1556 [00:17<00:00, 87.91it/s, loss=0.0407]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00       423\n","           1       0.89      0.82      0.85       402\n","           3       0.00      0.00      0.00        12\n","           4       0.00      0.00      0.00         4\n","           5       0.91      0.99      0.95      5516\n","\n","    accuracy                           0.91      6357\n","   macro avg       0.36      0.36      0.36      6357\n","weighted avg       0.85      0.91      0.88      6357\n","\n","Epoch 3/50 Summary: Train Loss: 0.2949 | Val Loss: 0.2854 | Val Acc: 0.9135\n","Skipped 3 batches due to NaN/Inf\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 4/50 [Train]:  48%|████▊     | 751/1556 [00:08<00:08, 95.39it/s, loss=0.324]"]},{"output_type":"stream","name":"stdout","text":["NaN or Inf detected in model output, skipping this batch\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 4/50 [Train]: 100%|██████████| 1556/1556 [00:17<00:00, 88.09it/s, loss=0.0183]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.56      0.01      0.02       423\n","           1       0.97      0.72      0.83       402\n","           3       0.00      0.00      0.00        12\n","           4       0.00      0.00      0.00         4\n","           5       0.91      1.00      0.95      5516\n","\n","    accuracy                           0.91      6357\n","   macro avg       0.49      0.35      0.36      6357\n","weighted avg       0.89      0.91      0.88      6357\n","\n","Epoch 4/50 Summary: Train Loss: 0.2808 | Val Loss: 0.2774 | Val Acc: 0.9124\n","Skipped 4 batches due to NaN/Inf\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 5/50 [Train]:  35%|███▌      | 547/1556 [00:06<00:11, 91.05it/s, loss=0.867]"]},{"output_type":"stream","name":"stdout","text":["NaN or Inf detected in model output, skipping this batch\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 5/50 [Train]: 100%|██████████| 1556/1556 [00:17<00:00, 87.17it/s, loss=0.138]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.50      0.00      0.01       423\n","           1       0.90      0.83      0.87       402\n","           3       0.00      0.00      0.00        12\n","           4       0.00      0.00      0.00         4\n","           5       0.92      0.99      0.95      5516\n","\n","    accuracy                           0.91      6357\n","   macro avg       0.46      0.37      0.37      6357\n","weighted avg       0.88      0.91      0.88      6357\n","\n","Epoch 5/50 Summary: Train Loss: 0.2692 | Val Loss: 0.3239 | Val Acc: 0.9147\n","Skipped 5 batches due to NaN/Inf\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 6/50 [Train]:  76%|███████▌  | 1175/1556 [00:13<00:04, 94.57it/s, loss=0.275]"]},{"output_type":"stream","name":"stdout","text":["NaN or Inf detected in model output, skipping this batch\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 6/50 [Train]: 100%|██████████| 1556/1556 [00:17<00:00, 87.13it/s, loss=0.52]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.50      0.04      0.07       423\n","           1       0.90      0.84      0.87       402\n","           3       0.00      0.00      0.00        12\n","           4       0.00      0.00      0.00         4\n","           5       0.92      0.99      0.95      5516\n","\n","    accuracy                           0.92      6357\n","   macro avg       0.46      0.37      0.38      6357\n","weighted avg       0.89      0.92      0.89      6357\n","\n","Epoch 6/50 Summary: Train Loss: 0.2664 | Val Loss: 0.2630 | Val Acc: 0.9155\n","Skipped 6 batches due to NaN/Inf\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 7/50 [Train]:  36%|███▌      | 563/1556 [00:06<00:11, 86.43it/s, loss=0.217]"]},{"output_type":"stream","name":"stdout","text":["NaN or Inf detected in model output, skipping this batch\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 7/50 [Train]: 100%|██████████| 1556/1556 [00:17<00:00, 88.86it/s, loss=0.456]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.59      0.06      0.11       423\n","           1       0.91      0.82      0.86       402\n","           3       0.00      0.00      0.00        12\n","           4       0.00      0.00      0.00         4\n","           5       0.92      0.99      0.95      5516\n","\n","    accuracy                           0.92      6357\n","   macro avg       0.48      0.37      0.38      6357\n","weighted avg       0.89      0.92      0.89      6357\n","\n","Epoch 7/50 Summary: Train Loss: 0.2593 | Val Loss: 0.2942 | Val Acc: 0.9157\n","Skipped 7 batches due to NaN/Inf\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 8/50 [Train]:  23%|██▎       | 351/1556 [00:03<00:10, 111.10it/s, loss=0.109] "]},{"output_type":"stream","name":"stdout","text":["NaN or Inf detected in model output, skipping this batch\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 8/50 [Train]: 100%|██████████| 1556/1556 [00:14<00:00, 107.26it/s, loss=0.419]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.48      0.12      0.20       423\n","           1       0.93      0.80      0.86       402\n","           3       0.00      0.00      0.00        12\n","           4       0.00      0.00      0.00         4\n","           5       0.92      0.99      0.95      5516\n","\n","    accuracy                           0.92      6357\n","   macro avg       0.47      0.38      0.40      6357\n","weighted avg       0.89      0.92      0.90      6357\n","\n","Epoch 8/50 Summary: Train Loss: 0.2479 | Val Loss: 0.2506 | Val Acc: 0.9154\n","Skipped 8 batches due to NaN/Inf\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 9/50 [Train]:  15%|█▌        | 240/1556 [00:02<00:11, 113.46it/s, loss=0.0422]"]},{"output_type":"stream","name":"stdout","text":["NaN or Inf detected in model output, skipping this batch\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 9/50 [Train]: 100%|██████████| 1556/1556 [00:14<00:00, 107.85it/s, loss=0.4]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.36      0.20      0.25       423\n","           1       0.88      0.87      0.87       402\n","           3       0.00      0.00      0.00        12\n","           4       0.00      0.00      0.00         4\n","           5       0.93      0.97      0.95      5516\n","\n","    accuracy                           0.91      6357\n","   macro avg       0.43      0.41      0.42      6357\n","weighted avg       0.89      0.91      0.90      6357\n","\n","Epoch 9/50 Summary: Train Loss: 0.2418 | Val Loss: 0.2641 | Val Acc: 0.9072\n","Skipped 9 batches due to NaN/Inf\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 10/50 [Train]:  30%|███       | 471/1556 [00:04<00:09, 111.12it/s, loss=0.0233]"]},{"output_type":"stream","name":"stdout","text":["NaN or Inf detected in model output, skipping this batch\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 10/50 [Train]: 100%|██████████| 1556/1556 [00:14<00:00, 107.40it/s, loss=0.14]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.55      0.09      0.15       423\n","           1       0.90      0.83      0.86       402\n","           3       0.00      0.00      0.00        12\n","           4       0.00      0.00      0.00         4\n","           5       0.92      0.99      0.95      5516\n","\n","    accuracy                           0.92      6357\n","   macro avg       0.47      0.38      0.39      6357\n","weighted avg       0.89      0.92      0.89      6357\n","\n","Epoch 10/50 Summary: Train Loss: 0.2419 | Val Loss: 0.2607 | Val Acc: 0.9166\n","Skipped 10 batches due to NaN/Inf\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 11/50 [Train]:  72%|███████▏  | 1115/1556 [00:10<00:03, 115.42it/s, loss=0.0744]"]},{"output_type":"stream","name":"stdout","text":["NaN or Inf detected in model output, skipping this batch\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 11/50 [Train]: 100%|██████████| 1556/1556 [00:14<00:00, 107.35it/s, loss=0.0805]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.46      0.15      0.22       423\n","           1       0.90      0.85      0.87       402\n","           3       0.00      0.00      0.00        12\n","           4       0.00      0.00      0.00         4\n","           5       0.93      0.98      0.95      5516\n","\n","    accuracy                           0.92      6357\n","   macro avg       0.46      0.40      0.41      6357\n","weighted avg       0.89      0.92      0.90      6357\n","\n","Epoch 11/50 Summary: Train Loss: 0.2356 | Val Loss: 0.2545 | Val Acc: 0.9157\n","Skipped 11 batches due to NaN/Inf\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 12/50 [Train]:  93%|█████████▎| 1448/1556 [00:16<00:01, 96.83it/s, loss=0.237]"]},{"output_type":"stream","name":"stdout","text":["NaN or Inf detected in model output, skipping this batch\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 12/50 [Train]: 100%|██████████| 1556/1556 [00:17<00:00, 89.97it/s, loss=0.101]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.36      0.23      0.28       423\n","           1       0.82      0.88      0.85       402\n","           3       0.00      0.00      0.00        12\n","           4       0.00      0.00      0.00         4\n","           5       0.94      0.96      0.95      5516\n","\n","    accuracy                           0.90      6357\n","   macro avg       0.42      0.41      0.42      6357\n","weighted avg       0.89      0.90      0.89      6357\n","\n","Epoch 12/50 Summary: Train Loss: 0.2289 | Val Loss: 0.2715 | Val Acc: 0.9036\n","Skipped 12 batches due to NaN/Inf\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 13/50 [Train]:  96%|█████████▌| 1497/1556 [00:18<00:00, 91.49it/s, loss=0.116]"]},{"output_type":"stream","name":"stdout","text":["NaN or Inf detected in model output, skipping this batch\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 13/50 [Train]: 100%|██████████| 1556/1556 [00:18<00:00, 83.09it/s, loss=0.193]\n","                                                                     "]},{"output_type":"stream","name":"stdout","text":["\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.46      0.13      0.20       423\n","           1       0.83      0.87      0.85       402\n","           3       0.00      0.00      0.00        12\n","           4       0.00      0.00      0.00         4\n","           5       0.93      0.98      0.95      5516\n","\n","    accuracy                           0.91      6357\n","   macro avg       0.44      0.39      0.40      6357\n","weighted avg       0.89      0.91      0.89      6357\n","\n","Epoch 13/50 Summary: Train Loss: 0.2262 | Val Loss: 0.2650 | Val Acc: 0.9130\n","Skipped 13 batches due to NaN/Inf\n","Early stopping triggered after 5 epochs without improvement.\n"]},{"output_type":"stream","name":"stderr","text":["\r"]}]},{"cell_type":"code","source":["import torch\n","import scipy.io as sio\n","from sklearn.metrics import classification_report\n","\n","def evaluate_model(model, data_loader, device='cpu', label_names=None, save_features=False, feature_layer='lstm', mat_file='features_by_file.mat'):\n","    model = model.to(device)\n","    model.eval()\n","\n","    all_preds = []\n","    all_labels = []\n","\n","    final_features = []\n","    final_filenames = []\n","\n","    with torch.no_grad():\n","        for x_batch, y_batch, lengths, filenames in data_loader:\n","            x_batch = x_batch.to(device)\n","            y_batch = y_batch.to(device)\n","\n","            outputs, lstm_out = model(x_batch, lengths)  # logits, lstm_out\n","            outputs = outputs.view(-1, outputs.shape[-1])\n","            y_batch = y_batch.view(-1)\n","\n","            preds = outputs.argmax(dim=-1)\n","            mask = y_batch != -100\n","\n","            all_preds.extend(preds[mask].cpu().numpy())\n","            all_labels.extend(y_batch[mask].cpu().numpy())\n","\n","            if save_features:\n","                # Choose layer to extract features from\n","                selected_features = lstm_out  # or change to `linear_1024_out` if you modify model to return it\n","                selected_features = selected_features.cpu()\n","\n","                for i, fname in enumerate(filenames):\n","                    seq_len = lengths[i]\n","                    features = selected_features[i][:seq_len]  # [seq_len, feat_dim]\n","                    # features_dict[fname] = features.numpy()\n","                    final_features.append(features.numpy())\n","                    final_filenames.append(fname)\n","\n","    if save_features:\n","        features_dict = {'filenames': final_filenames, 'features': final_features}\n","\n","    print(\"📊 Per-Class Metrics:\\n\")\n","    report = classification_report(\n","        all_labels, all_preds, target_names=label_names, digits=4, zero_division=0\n","    )\n","    print(report)\n","\n","    if save_features:\n","        print(f\"💾 Saving per-file features to: {mat_file}\")\n","        sio.savemat(mat_file, features_dict)\n"],"metadata":{"id":"ngt47Bud0ASc","executionInfo":{"status":"ok","timestamp":1746208760668,"user_tz":-330,"elapsed":70,"user":{"displayName":"Sai Harshitha Aluru","userId":"02296733820851712734"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["# torch.save(model.state_dict(), \"/content/drive/MyDrive/SSMT/speech_punc_extraction_model.pt\")"],"metadata":{"id":"oHBOawcw2B8c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Assuming 6 classes and you know the labels\n","label_names = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\"]\n","\n","# Load the best model weights (optional if not already loaded)\n","model.load_state_dict(torch.load('/content/drive/MyDrive/SSMT/speech_punc_extraction_model.pt'))\n","\n","# Specify where you want to save the features (e.g., 'train_features.mat')\n","mat_file_path = '/content/drive/MyDrive/SSMT/final_train_punct_features_speech.mat'\n","\n","# Evaluate on training set (or val_loader) and save features\n","evaluate_model(model, train_loader, device='cuda', label_names=label_names, save_features=True, mat_file=mat_file_path)\n","\n","label_names = [\"0\", \"1\", \"2\", \"3\", \"4\"]\n","\n","mat_file_path = '/content/drive/MyDrive/SSMT/final_dev_punct_features_speech.mat'\n","\n","evaluate_model(model, val_loader, device='cuda', label_names=label_names, save_features=True, mat_file=mat_file_path)\n","\n","label_names = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\"]\n","\n","mat_file_path = '/content/drive/MyDrive/SSMT/final_test_punct_features_speech.mat'\n","\n","evaluate_model(model, test_loader, device='cuda', label_names=label_names, save_features=True, mat_file=mat_file_path)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0TCbmDj518T4","executionInfo":{"status":"ok","timestamp":1746208774642,"user_tz":-330,"elapsed":8219,"user":{"displayName":"Sai Harshitha Aluru","userId":"02296733820851712734"}},"outputId":"47eb42b0-2f31-43d7-cc3d-40f09a33f8e3"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["📊 Per-Class Metrics:\n","\n","              precision    recall  f1-score   support\n","\n","           0     0.6324    0.1663    0.2634      3776\n","           1     0.9420    0.7726    0.8490      4165\n","           2     0.0000    0.0000    0.0000        38\n","           3     0.0000    0.0000    0.0000       177\n","           4     0.0000    0.0000    0.0000        88\n","           5     0.8879    0.9884    0.9355     33891\n","\n","    accuracy                         0.8863     42135\n","   macro avg     0.4104    0.3212    0.3413     42135\n","weighted avg     0.8640    0.8863    0.8600     42135\n","\n","💾 Saving per-file features to: /content/drive/MyDrive/SSMT/final_train_punct_features_speech.mat\n","📊 Per-Class Metrics:\n","\n","              precision    recall  f1-score   support\n","\n","           0     0.5105    0.1052    0.1744       694\n","           1     0.9428    0.7583    0.8405       695\n","           2     0.0000    0.0000    0.0000        31\n","           3     0.0000    0.0000    0.0000         6\n","           4     0.8715    0.9859    0.9251      5516\n","\n","    accuracy                         0.8698      6942\n","   macro avg     0.4649    0.3699    0.3880      6942\n","weighted avg     0.8379    0.8698    0.8367      6942\n","\n","💾 Saving per-file features to: /content/drive/MyDrive/SSMT/final_dev_punct_features_speech.mat\n","📊 Per-Class Metrics:\n","\n","              precision    recall  f1-score   support\n","\n","           0     0.5130    0.0938    0.1585      1056\n","           1     0.9615    0.6888    0.8026      1160\n","           2     0.0000    0.0000    0.0000         4\n","           3     0.0000    0.0000    0.0000        91\n","           4     0.0000    0.0000    0.0000        34\n","           5     0.8646    0.9903    0.9232      9083\n","\n","    accuracy                         0.8657     11428\n","   macro avg     0.3898    0.2955    0.3141     11428\n","weighted avg     0.8322    0.8657    0.8299     11428\n","\n","💾 Saving per-file features to: /content/drive/MyDrive/SSMT/final_test_punct_features_speech.mat\n"]}]},{"cell_type":"code","source":["import scipy.io\n","\n","# Load the .mat file\n","mat_file_path = '/content/drive/MyDrive/SSMT/final_train_punct_features_speech.mat'  # Replace with your file path\n","mat_data = scipy.io.loadmat(mat_file_path)\n","\n","# Display the keys in the .mat file\n","print(\"Keys in the .mat file:\", mat_data.keys())\n","\n","# If the features are stored in a specific variable, you can access it\n","# For example, if the features are stored under 'features' key\n","if 'features' in mat_data:\n","    features = mat_data['features']\n","    print(\"Shape of the features:\", features.shape)\n","else:\n","    print(\"No 'features' key found in the .mat file.\")\n","\n","features = mat_data['features'].T[1][0]\n","print(features.shape)\n","\n","\n","\n","\n","# Load the .mat file\n","mat_file_path = '/content/drive/MyDrive/SSMT/final_dev_punct_features_speech.mat'  # Replace with your file path\n","mat_data = scipy.io.loadmat(mat_file_path)\n","\n","# Display the keys in the .mat file\n","print(\"Keys in the .mat file:\", mat_data.keys())\n","\n","# If the features are stored in a specific variable, you can access it\n","# For example, if the features are stored under 'features' key\n","if 'features' in mat_data:\n","    features = mat_data['features']\n","    print(\"Shape of the features:\", features.shape)\n","else:\n","    print(\"No 'features' key found in the .mat file.\")\n","\n","features = mat_data['features'].T[1][0]\n","print(features.shape)\n","\n","\n","\n","\n","# Load the .mat file\n","mat_file_path = '/content/drive/MyDrive/SSMT/final_test_punct_features_speech.mat'  # Replace with your file path\n","mat_data = scipy.io.loadmat(mat_file_path)\n","\n","# Display the keys in the .mat file\n","print(\"Keys in the .mat file:\", mat_data.keys())\n","\n","# If the features are stored in a specific variable, you can access it\n","# For example, if the features are stored under 'features' key\n","if 'features' in mat_data:\n","    features = mat_data['features']\n","    print(\"Shape of the features:\", features.shape)\n","else:\n","    print(\"No 'features' key found in the .mat file.\")\n","\n","features = mat_data['features'].T[1][0]\n","print(features.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2v2SQfyR2AhL","executionInfo":{"status":"ok","timestamp":1746208779742,"user_tz":-330,"elapsed":351,"user":{"displayName":"Sai Harshitha Aluru","userId":"02296733820851712734"}},"outputId":"ebab533d-a175-4842-e883-89442a0bda9f"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["Keys in the .mat file: dict_keys(['__header__', '__version__', '__globals__', 'filenames', 'features'])\n","Shape of the features: (1, 1545)\n","(30, 1024)\n","Keys in the .mat file: dict_keys(['__header__', '__version__', '__globals__', 'filenames', 'features'])\n","Shape of the features: (1, 255)\n","(34, 1024)\n","Keys in the .mat file: dict_keys(['__header__', '__version__', '__globals__', 'filenames', 'features'])\n","Shape of the features: (1, 399)\n","(24, 1024)\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"taID7LnYAl46"},"execution_count":null,"outputs":[]}]}
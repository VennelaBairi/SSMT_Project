{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import gdown\n\ncsv_file_id = '1-DfbuRqP5xe9eNMjobEDH_cZR1FlwyEw'\ngdown.download(f'https://drive.google.com/uc?export=download&id={csv_file_id}', 'MT_data.csv', quiet=False)\ntsv_file_id = '1pBYaqKbkJt66bJlBJGKvTaWVI6Q0yNCC'\ngdown.download(f'https://drive.google.com/uc?export=download&id={tsv_file_id}', 'eng_data.tsv', quiet=False)\nzip_file_id = '1aNBV7Jgpgm1d21IGnbsIB0E35kZbb0Sc'\ngdown.download(f'https://drive.google.com/uc?export=download&id={zip_file_id}', 'punctuation_data.zip', quiet=False)\n\ncsv_file_id = '1-I-VFAdiAvFF-7x_Y_0_TwDf0v2kl80l'\ngdown.download(f'https://drive.google.com/uc?export=download&id={csv_file_id}', 'MT_dev.csv', quiet=False)\ntsv_file_id = '1M6uGyGSOEW7wKirzi7dlizlRaN9wStwD'\ngdown.download(f'https://drive.google.com/uc?export=download&id={tsv_file_id}', 'dev.tsv', quiet=False)\nzip_file_id = '1w4FR5I3GUp3cq1I-LIGREWRufLiwrAVU'\ngdown.download(f'https://drive.google.com/uc?export=download&id={zip_file_id}', 'punctuation_dev.zip', quiet=False)\n\ncsv_file_id = '1-GVAKGJs92uzo8bF1d-q9J_apyqP1ofG'\ngdown.download(f'https://drive.google.com/uc?export=download&id={csv_file_id}', 'MT_test.csv', quiet=False)\ntsv_file_id = '1m7JIcj_-SVYASbcbBiT4LoPdgZdqGN4i'\ngdown.download(f'https://drive.google.com/uc?export=download&id={tsv_file_id}', 'test.tsv', quiet=False)\nzip_file_id = '1Onbil_LmG8MSFPwGPRe5-8iuqFMrRBv5'\ngdown.download(f'https://drive.google.com/uc?export=download&id={zip_file_id}', 'punctuation_test.zip', quiet=False)\n\nzip_file_id = '10MA1p4EGfU2zIGIG9j3ZesA73oTVR61-'\ngdown.download(f'https://drive.google.com/uc?export=download&id={zip_file_id}', 'punctuation_dev_asr.zip', quiet=False)\n\nzip_file_id = '1NoxnboKl2H_bXn_JeaRIlsDBv_CotfLD'\ngdown.download(f'https://drive.google.com/uc?export=download&id={zip_file_id}', 'punctuation_test_asr.zip', quiet=False)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-05T11:32:30.718850Z","iopub.execute_input":"2025-05-05T11:32:30.719406Z","iopub.status.idle":"2025-05-05T11:33:18.096749Z","shell.execute_reply.started":"2025-05-05T11:32:30.719380Z","shell.execute_reply":"2025-05-05T11:33:18.096115Z"}},"outputs":[{"name":"stderr","text":"Downloading...\nFrom: https://drive.google.com/uc?export=download&id=1-DfbuRqP5xe9eNMjobEDH_cZR1FlwyEw\nTo: /kaggle/working/MT_data.csv\n100%|██████████| 599k/599k [00:00<00:00, 99.5MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?export=download&id=1pBYaqKbkJt66bJlBJGKvTaWVI6Q0yNCC\nTo: /kaggle/working/eng_data.tsv\n100%|██████████| 1.41M/1.41M [00:00<00:00, 100MB/s]\nDownloading...\nFrom (original): https://drive.google.com/uc?export=download&id=1aNBV7Jgpgm1d21IGnbsIB0E35kZbb0Sc\nFrom (redirected): https://drive.google.com/uc?export=download&id=1aNBV7Jgpgm1d21IGnbsIB0E35kZbb0Sc&confirm=t&uuid=4313fac5-9464-496d-901f-0dd503f2b2f6\nTo: /kaggle/working/punctuation_data.zip\n100%|██████████| 333M/333M [00:03<00:00, 101MB/s]  \nDownloading...\nFrom: https://drive.google.com/uc?export=download&id=1-I-VFAdiAvFF-7x_Y_0_TwDf0v2kl80l\nTo: /kaggle/working/MT_dev.csv\n100%|██████████| 66.2k/66.2k [00:00<00:00, 44.2MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?export=download&id=1M6uGyGSOEW7wKirzi7dlizlRaN9wStwD\nTo: /kaggle/working/dev.tsv\n100%|██████████| 213k/213k [00:00<00:00, 26.7MB/s]\nDownloading...\nFrom (original): https://drive.google.com/uc?export=download&id=1w4FR5I3GUp3cq1I-LIGREWRufLiwrAVU\nFrom (redirected): https://drive.google.com/uc?export=download&id=1w4FR5I3GUp3cq1I-LIGREWRufLiwrAVU&confirm=t&uuid=6eabf5bc-66dc-47f9-aba8-23ba9e0da0fa\nTo: /kaggle/working/punctuation_dev.zip\n100%|██████████| 47.4M/47.4M [00:00<00:00, 90.1MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?export=download&id=1-GVAKGJs92uzo8bF1d-q9J_apyqP1ofG\nTo: /kaggle/working/MT_test.csv\n100%|██████████| 148k/148k [00:00<00:00, 19.8MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?export=download&id=1m7JIcj_-SVYASbcbBiT4LoPdgZdqGN4i\nTo: /kaggle/working/test.tsv\n100%|██████████| 368k/368k [00:00<00:00, 104MB/s]\nDownloading...\nFrom (original): https://drive.google.com/uc?export=download&id=1Onbil_LmG8MSFPwGPRe5-8iuqFMrRBv5\nFrom (redirected): https://drive.google.com/uc?export=download&id=1Onbil_LmG8MSFPwGPRe5-8iuqFMrRBv5&confirm=t&uuid=373ce1a0-31ec-4e78-966f-a8bb9f0eec10\nTo: /kaggle/working/punctuation_test.zip\n100%|██████████| 81.3M/81.3M [00:00<00:00, 93.1MB/s]\nDownloading...\nFrom (original): https://drive.google.com/uc?export=download&id=10MA1p4EGfU2zIGIG9j3ZesA73oTVR61-\nFrom (redirected): https://drive.google.com/uc?export=download&id=10MA1p4EGfU2zIGIG9j3ZesA73oTVR61-&confirm=t&uuid=3c48b1ef-bf35-4452-8183-c686dde00d4f\nTo: /kaggle/working/punctuation_dev_asr.zip\n100%|██████████| 42.8M/42.8M [00:01<00:00, 33.2MB/s]\nDownloading...\nFrom (original): https://drive.google.com/uc?export=download&id=1NoxnboKl2H_bXn_JeaRIlsDBv_CotfLD\nFrom (redirected): https://drive.google.com/uc?export=download&id=1NoxnboKl2H_bXn_JeaRIlsDBv_CotfLD&confirm=t&uuid=803ea6ef-ddb9-465c-a799-f04c03b05374\nTo: /kaggle/working/punctuation_test_asr.zip\n100%|██████████| 67.2M/67.2M [00:00<00:00, 145MB/s]\n","output_type":"stream"},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"'punctuation_test_asr.zip'"},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"import os\nimport zipfile\nimport torch\nimport pandas as pd\nfrom transformers import MBartTokenizer\n\nclass EnglishTeluguPunctDataset:\n    def __init__(self, zip_path, tsv_path, csv_path, tokenizer, max_length=128):\n        self.zip_path = zip_path\n        self.tsv_path = tsv_path\n        self.csv_path = csv_path\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.skipped = 0\n\n        # Load MT_data.csv (contains both English and Telugu)\n        mt_data = pd.read_csv(csv_path)\n        self.id_to_english = mt_data.set_index(\"id\")[\"english\"].to_dict()\n        self.id_to_telugu = mt_data.set_index(\"id\")[\"telugu\"].to_dict()\n\n        # Load eng_data.tsv (only for filename to ID mapping)\n        eng_tsv = pd.read_csv(tsv_path, sep=\"\\t\", header=None, dtype=str)\n        eng_tsv = eng_tsv[eng_tsv.columns[:2]]\n        eng_tsv.columns = [\"id\", \"filename\"]\n        eng_tsv[\"id\"] = eng_tsv[\"id\"].astype(int)\n        self.filename_to_id = eng_tsv.set_index(\"filename\")[\"id\"].to_dict()\n\n        # Read .pt filenames from ZIP\n        with zipfile.ZipFile(zip_path, 'r') as zipf:\n            self.pt_filenames = [f for f in zipf.namelist() if f.endswith('.pt')]\n\n        # Match each .pt file to a dataset entry\n        self.valid_entries = []\n        for pt_file in self.pt_filenames:\n            base = os.path.basename(pt_file)\n            filename_without_extension = base.replace(\".pt\", \"\")\n\n            if filename_without_extension in self.filename_to_id:\n                id_ = self.filename_to_id[filename_without_extension]\n                if id_ in self.id_to_english and id_ in self.id_to_telugu:\n                    self.valid_entries.append((pt_file, id_))\n                else:\n                    self.skipped += 1\n            else:\n                self.skipped += 1\n\n    def __len__(self):\n        return len(self.valid_entries)\n\n    def __getitem__(self, idx):\n        pt_file, id_ = self.valid_entries[idx]\n    \n        # Load punctuation features from .pt file\n        with zipfile.ZipFile(self.zip_path, 'r') as zipf:\n            with zipf.open(pt_file) as f:\n                punct_feats = torch.load(f, weights_only=True)\n    \n        # Get English and Telugu text\n        src_text = self.id_to_english[id_]\n        tgt_text = self.id_to_telugu[id_]\n    \n        # Tokenize without max_length padding\n        src_enc = self.tokenizer(src_text, return_tensors=\"pt\", padding=False, truncation=True)\n        tgt_enc = self.tokenizer(tgt_text, return_tensors=\"pt\", padding=False, truncation=True)\n    \n        # Set language codes as first token\n        src_lang = \"en_XX\"\n        tgt_lang = \"te_IN\"\n        src_enc.input_ids[0][0] = self.tokenizer.lang_code_to_id[src_lang]\n        tgt_enc.input_ids[0][0] = self.tokenizer.lang_code_to_id[tgt_lang]\n    \n        return {\n            \"input_ids\": src_enc.input_ids.squeeze(0),\n            \"attention_mask\": src_enc.attention_mask.squeeze(0),\n            \"labels\": tgt_enc.input_ids.squeeze(0),\n            \"punct_feats\": punct_feats,\n            \"src\": src_text,\n            \"tgt\": tgt_text,\n            \"filename\": os.path.basename(pt_file).replace(\".pt\", \"\"),\n            \"id\": id_\n        }\n\n    def get_skipped_count(self):\n        return self.skipped\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T11:33:18.097957Z","iopub.execute_input":"2025-05-05T11:33:18.098363Z","iopub.status.idle":"2025-05-05T11:33:20.686411Z","shell.execute_reply.started":"2025-05-05T11:33:18.098342Z","shell.execute_reply":"2025-05-05T11:33:20.685604Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom transformers import MBartForConditionalGeneration, MBart50TokenizerFast\nfrom torch.optim import AdamW\nfrom tqdm import tqdm\nfrom transformers.modeling_outputs import BaseModelOutput\nfrom torch.utils.data import DataLoader\n\n\n\nmodel_name = \"facebook/mbart-large-50-many-to-many-mmt\"\n# Load tokenizer and model\n# tokenizer = MBart50TokenizerFast.from_pretrained(model_name)\nmodel = MBartForConditionalGeneration.from_pretrained(model_name)\n\n# Initialize the tokenizer\ntokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n\n# Initialize the dataset\ntrain_dataset = EnglishTeluguPunctDataset(zip_path=\"punctuation_data.zip\",\n                                    tsv_path=\"eng_data.tsv\",\n                                    csv_path=\"MT_data.csv\",\n                                    tokenizer=tokenizer)\n# Create DataLoader\ntrain_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n\n# Initialize the dataset\ndev_dataset = EnglishTeluguPunctDataset(zip_path=\"punctuation_dev.zip\",\n                                    tsv_path=\"dev.tsv\",\n                                    csv_path=\"MT_dev.csv\",\n                                    tokenizer=tokenizer)\n\n# Create DataLoader\ndev_dataloader = DataLoader(dev_dataset, batch_size=1, shuffle=True)\n\n\n\n# Initialize the dataset\ntest_dataset = EnglishTeluguPunctDataset(zip_path=\"punctuation_test.zip\",\n                                    tsv_path=\"test.tsv\",\n                                    csv_path=\"MT_test.csv\",\n                                    tokenizer=tokenizer)\n\n# Create DataLoader\ntest_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=True)\n\n# ASR\n\n\ndev_asr_dataset = EnglishTeluguPunctDataset(zip_path=\"punctuation_dev_asr.zip\",\n                                    tsv_path=\"dev.tsv\",\n                                    csv_path=\"MT_dev.csv\",\n                                    tokenizer=tokenizer)\n\n# Create DataLoader\ndev_asr_dataloader = DataLoader(dev_dataset, batch_size=1, shuffle=True)\n\n\n\n# Initialize the dataset\ntest_asr_dataset = EnglishTeluguPunctDataset(zip_path=\"punctuation_test_asr.zip\",\n                                    tsv_path=\"test.tsv\",\n                                    csv_path=\"MT_test.csv\",\n                                    tokenizer=tokenizer)\n\n# Create DataLoader\ntest_asr_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T11:33:20.687263Z","iopub.execute_input":"2025-05-05T11:33:20.687713Z","iopub.status.idle":"2025-05-05T11:33:31.116837Z","shell.execute_reply.started":"2025-05-05T11:33:20.687688Z","shell.execute_reply":"2025-05-05T11:33:31.116265Z"}},"outputs":[{"name":"stderr","text":"2025-05-05 11:33:23.132192: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1746444803.154487     187 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1746444803.161200     187 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"SRC_LANG = \"en_XX\"\nTGT_LANG = \"te_IN\"\n\ntokenizer.src_lang = SRC_LANG\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Set model to training mode\nmodel.train()\n\n# Create the DataLoader (assuming your dataset is already created)\n# dataset = EnglishTeluguPunctDataset(...)\n# Optimizer\noptimizer = AdamW(model.parameters(), lr=5e-5)\n\nimport torch.nn as nn\n\nfrom transformers.modeling_outputs import BaseModelOutput\nskip=0\n\nimport torch.nn as nn\n\n# Assume punct_feats shape: (batch_size, seq_len, feat_dim)\nfeat_dim = train_dataset[0][\"punct_feats\"].shape[1]\nhidden_dim = model.model.encoder.config.d_model  # mBART hidden size (e.g., 1024)\n\n# Punctuation feature encoder\npunct_feat_encoder = nn.Linear(feat_dim, hidden_dim).to(device)\n\n# Add to optimizer\noptimizer = AdamW(list(model.parameters()) + list(punct_feat_encoder.parameters()), lr=5e-5)\n\n\n# for epoch in range(3):  # Example: 3 epochs\n#     model.train()\n#     for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}\"):\n#         optimizer.zero_grad()\n        \n#         input_ids = batch['input_ids'].to(device)\n#         attention_mask = batch['attention_mask'].to(device)\n#         labels = batch['labels'].to(device)\n#         punct_feats = batch['punct_feats'].to(device)  # (batch, seq_len, feat_dim)\n\n#         # Get encoder output from mBART\n#         encoder_outputs = model.model.encoder(\n#             input_ids=input_ids,\n#             attention_mask=attention_mask,\n#             return_dict=True\n#         )\n\n#         try:\n#             # Add punctuation features to encoder hidden states\n#             combined_hidden_state = encoder_outputs.last_hidden_state + punct_feats * 0.1\n#         except:\n#             skip+=1\n#             continue\n\n#         # Wrap into BaseModelOutput\n#         encoder_outputs = BaseModelOutput(last_hidden_state=combined_hidden_state)\n\n#         decoder_input_ids = model.prepare_decoder_input_ids_from_labels(labels)\n\n#         # outputs = model(\n#         #     input_ids=None,\n#         #     attention_mask=None,\n#         #     encoder_outputs=encoder_outputs,\n#         #     decoder_input_ids=decoder_input_ids,\n#         #     labels=labels\n#         # )\n\n#         # Forward pass through the full model\n#         outputs = model(\n#             input_ids=None,\n#             attention_mask=None,\n#             decoder_input_ids=decoder_input_ids,  # Use left-shifted target if needed\n#             encoder_outputs=encoder_outputs,\n#             labels=labels,\n#             return_dict=True\n#         )\n\n#         # Backpropagation\n#         loss = outputs.loss\n#         loss.backward()\n#         optimizer.step()\n\n#     print(f\"Epoch {epoch + 1}, Loss: {loss.item():.4f}\")\n\nfor epoch in range(3):\n    model.train()\n    punct_feat_encoder.train()\n\n    for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}\"):\n        optimizer.zero_grad()\n\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n        punct_feats = batch['punct_feats'].to(device)  # (batch, seq_len, feat_dim)\n\n        # Forward encoder\n        encoder_outputs = model.model.encoder(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            return_dict=True\n        )\n\n        # Pass punctuation features through its encoder\n        try:\n            encoded_punct_feats = punct_feat_encoder(punct_feats)  # (batch, seq_len, hidden_dim)\n            combined_hidden_state = encoder_outputs.last_hidden_state + encoded_punct_feats\n        except Exception as e:\n            skip += 1\n            continue\n\n        encoder_outputs = BaseModelOutput(last_hidden_state=combined_hidden_state)\n        decoder_input_ids = model.prepare_decoder_input_ids_from_labels(labels)\n\n        outputs = model(\n            input_ids=None,\n            attention_mask=None,\n            decoder_input_ids=decoder_input_ids,\n            encoder_outputs=encoder_outputs,\n            labels=labels,\n            return_dict=True\n        )\n\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n\n    print(f\"Epoch {epoch + 1}, Loss: {loss.item():.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save model and tokenizer to a directory\nsave_path = \"mbart_en_te_text_punc_only\"\nmodel.save_pretrained(save_path)\ntokenizer.save_pretrained(save_path)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\n\n# Directory to zip\nsave_path = \"mbart_en_te_text_punc_only\"\n\n# Create a zip file (it will be named 'mbart_en_te_text_punc_only.zip')\nshutil.make_archive(save_path, 'zip', save_path)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.save(model.state_dict(), \"mbart_en_te_text_punc_only.pt\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(translations[:10])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import gdown\nmodel_id = '13djMIYgy7etmFcD7A31x4ucGxgQ-NACc'\ngdown.download(f'https://drive.google.com/uc?export=download&id={model_id}', 'mbart_en_te_text_punc_with_encoder_only1.pt', quiet=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T11:33:31.118510Z","iopub.execute_input":"2025-05-05T11:33:31.118764Z","iopub.status.idle":"2025-05-05T11:33:48.852440Z","shell.execute_reply.started":"2025-05-05T11:33:31.118745Z","shell.execute_reply":"2025-05-05T11:33:48.851584Z"}},"outputs":[{"name":"stderr","text":"Downloading...\nFrom (original): https://drive.google.com/uc?export=download&id=13djMIYgy7etmFcD7A31x4ucGxgQ-NACc\nFrom (redirected): https://drive.google.com/uc?export=download&id=13djMIYgy7etmFcD7A31x4ucGxgQ-NACc&confirm=t&uuid=9ab43ff8-66e9-4555-a34b-2a33640db8b6\nTo: /kaggle/working/mbart_en_te_text_punc_with_encoder_only1.pt\n100%|██████████| 2.44G/2.44G [00:13<00:00, 182MB/s] \n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"'mbart_en_te_text_punc_with_encoder_only1.pt'"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"state_dict = torch.load('mbart_en_te_text_punc_with_encoder_only1.pt', weights_only=True)\nmodel.load_state_dict(state_dict)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T11:33:48.853325Z","iopub.execute_input":"2025-05-05T11:33:48.853643Z","iopub.status.idle":"2025-05-05T11:33:54.203471Z","shell.execute_reply.started":"2025-05-05T11:33:48.853612Z","shell.execute_reply":"2025-05-05T11:33:54.202802Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T11:33:54.204102Z","iopub.execute_input":"2025-05-05T11:33:54.204394Z","iopub.status.idle":"2025-05-05T11:33:55.020755Z","shell.execute_reply.started":"2025-05-05T11:33:54.204374Z","shell.execute_reply":"2025-05-05T11:33:55.020070Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"MBartForConditionalGeneration(\n  (model): MBartModel(\n    (shared): MBartScaledWordEmbedding(250054, 1024, padding_idx=1)\n    (encoder): MBartEncoder(\n      (embed_tokens): MBartScaledWordEmbedding(250054, 1024, padding_idx=1)\n      (embed_positions): MBartLearnedPositionalEmbedding(1026, 1024)\n      (layers): ModuleList(\n        (0-11): 12 x MBartEncoderLayer(\n          (self_attn): MBartSdpaAttention(\n            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (activation_fn): ReLU()\n          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n    )\n    (decoder): MBartDecoder(\n      (embed_tokens): MBartScaledWordEmbedding(250054, 1024, padding_idx=1)\n      (embed_positions): MBartLearnedPositionalEmbedding(1026, 1024)\n      (layers): ModuleList(\n        (0-11): 12 x MBartDecoderLayer(\n          (self_attn): MBartSdpaAttention(\n            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          )\n          (activation_fn): ReLU()\n          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (encoder_attn): MBartSdpaAttention(\n            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          )\n          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n    )\n  )\n  (lm_head): Linear(in_features=1024, out_features=250054, bias=False)\n)"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\nimport torch\nimport pandas as pd\nimport csv\n\n# Assuming model and tokenizer are already loaded\n# model = MBartForConditionalGeneration.from_pretrained(model_path)\n# tokenizer = MBart50TokenizerFast.from_pretrained(model_path)\n\nmodel.eval()\n\n# Define the source and target languages\nSRC_LANG = \"en_XX\"\nTGT_LANG = \"te_IN\"\n\n# Set the tokenizer source language\ntokenizer.src_lang = SRC_LANG\n\n# Placeholder for storing translations\ntranslations = []\n\n# Prepare CSV file (write header once)\noutput_csv = \"dev_translations.csv\"\nwith open(output_csv, mode='w', newline='', encoding='utf-8') as f:\n    writer = csv.DictWriter(f, fieldnames=[\"filename\", \"id\", \"src\", \"tgt\", \"translated\"])\n    writer.writeheader()\n\n    skip2=0\n    # Iterate over the dev_loader\n    for batch in tqdm(dev_dataloader, desc=\"Processing test data\", unit=\"batch\"):\n        # Assuming 'input_ids' and 'attention_mask' are present in the batch\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        punct_feats = batch['punct_feats'].to(device)\n        \n        # Unwrap values from lists or tensors\n        filename = batch['filename'][0] if isinstance(batch['filename'], list) else batch['filename']\n        fileid = batch['id'].item() if isinstance(batch['id'], torch.Tensor) else batch['id']\n        src_text = batch['src'][0] if isinstance(batch['src'], list) else batch['src']\n        tgt_text = batch['tgt'][0] if isinstance(batch['tgt'], list) else batch['tgt']\n\n        \n        # Set the language token for the source\n        input_ids[0][0] = tokenizer.lang_code_to_id[SRC_LANG]\n        \n        # Run the encoder manually\n        with torch.no_grad():\n            encoder_outputs = model.model.encoder(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                return_dict=True\n            )\n    \n        try:\n            # Add punctuation features to encoder hidden states\n            combined_hidden_state = encoder_outputs.last_hidden_state\n        except:\n            skip2+=1\n            # print('skipping :(')\n            continue\n    \n        # Wrap into BaseModelOutput\n        encoder_outputs = BaseModelOutput(last_hidden_state=combined_hidden_state)\n    \n        # Generate translation from decoder with forced beginning token for Telugu\n        generated_tokens = model.generate(\n            input_ids=None,\n            encoder_outputs=encoder_outputs,\n            attention_mask=None,\n            forced_bos_token_id=tokenizer.lang_code_to_id[TGT_LANG],\n            max_length=50\n        )\n        \n        # Decode the generated tokens into text\n        translated_text = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n        translations.append(translated_text)\n    \n        writer.writerow({\n                    \"filename\": filename,\n                    \"id\": fileid,\n                    \"src\": src_text,\n                    \"tgt\": tgt_text,\n                    \"translated\": translated_text\n                })\n    \n    \n    # Print out translations for the entire dev set\n    for translated_text in translations:\n        print(translated_text)\n        break\n    \n    print(skip2)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df= pd.read_csv(\"dev_translations.csv\")\nprint(df.head())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\nimport torch\nimport pandas as pd\nimport csv\n\n# Assuming model and tokenizer are already loaded\n# model = MBartForConditionalGeneration.from_pretrained(model_path)\n# tokenizer = MBart50TokenizerFast.from_pretrained(model_path)\n\nmodel.eval()\n\n# Define the source and target languages\nSRC_LANG = \"en_XX\"\nTGT_LANG = \"te_IN\"\n\n# Set the tokenizer source language\ntokenizer.src_lang = SRC_LANG\n\n# Placeholder for storing translations\ntranslations = []\n\n# Prepare CSV file (write header once)\noutput_csv = \"text_test_translations_2.csv\"\nwith open(output_csv, mode='w', newline='', encoding='utf-8') as f:\n    writer = csv.DictWriter(f, fieldnames=[\"filename\", \"id\", \"src\", \"tgt\", \"translated\"])\n    writer.writeheader()\n\n    skip2=0\n    # Iterate over the dev_loader\n    for batch in tqdm(test_dataloader, desc=\"Processing test data\", unit=\"batch\"):\n        # Assuming 'input_ids' and 'attention_mask' are present in the batch\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        punct_feats = batch['punct_feats'].to(device)\n        \n        # Unwrap values from lists or tensors\n        filename = batch['filename'][0] if isinstance(batch['filename'], list) else batch['filename']\n        fileid = batch['id'].item() if isinstance(batch['id'], torch.Tensor) else batch['id']\n        src_text = batch['src'][0] if isinstance(batch['src'], list) else batch['src']\n        tgt_text = batch['tgt'][0] if isinstance(batch['tgt'], list) else batch['tgt']\n\n        \n        # Set the language token for the source\n        input_ids[0][0] = tokenizer.lang_code_to_id[SRC_LANG]\n        \n        # Run the encoder manually\n        with torch.no_grad():\n            encoder_outputs = model.model.encoder(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                return_dict=True\n            )\n    \n        try:\n            # Add punctuation features to encoder hidden states\n            combined_hidden_state = encoder_outputs.last_hidden_state\n        except:\n            skip2+=1\n            # print('skipping :(')\n            continue\n    \n        # Wrap into BaseModelOutput\n        encoder_outputs = BaseModelOutput(last_hidden_state=combined_hidden_state)\n    \n        # Generate translation from decoder with forced beginning token for Telugu\n        generated_tokens = model.generate(\n            input_ids=None,\n            encoder_outputs=encoder_outputs,\n            attention_mask=None,\n            forced_bos_token_id=tokenizer.lang_code_to_id[TGT_LANG],\n            max_length=50\n        )\n        \n        # Decode the generated tokens into text\n        translated_text = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n        translations.append(translated_text)\n    \n        writer.writerow({\n                    \"filename\": filename,\n                    \"id\": fileid,\n                    \"src\": src_text,\n                    \"tgt\": tgt_text,\n                    \"translated\": translated_text\n                })\n    \n    \n    # Print out translations for the entire dev set\n    for translated_text in translations:\n        print(translated_text)\n        break\n    \n    print(skip2)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df= pd.read_csv(\"test_translations.csv\")\nprint(df.head())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"csv_file_id = '1FR-MyJRxZH62grW7DihFqbgA8HG0Wnne'\ngdown.download(f'https://drive.google.com/uc?export=download&id={csv_file_id}', 'asr_decoded_dev.csv', quiet=False)\n\n#https://drive.google.com/file/d/1e0JBRmo98zKK8Lbfof9QFZ7IlQiYtDC2/view?usp=drive_link\ncsv_file_id = '1e0JBRmo98zKK8Lbfof9QFZ7IlQiYtDC2'\ngdown.download(f'https://drive.google.com/uc?export=download&id={csv_file_id}', 'asr_decoded_test.csv', quiet=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T11:33:55.021508Z","iopub.execute_input":"2025-05-05T11:33:55.021731Z","iopub.status.idle":"2025-05-05T11:34:05.887839Z","shell.execute_reply.started":"2025-05-05T11:33:55.021713Z","shell.execute_reply":"2025-05-05T11:34:05.887173Z"}},"outputs":[{"name":"stderr","text":"Downloading...\nFrom: https://drive.google.com/uc?export=download&id=1FR-MyJRxZH62grW7DihFqbgA8HG0Wnne\nTo: /kaggle/working/asr_decoded_dev.csv\n100%|██████████| 227k/227k [00:00<00:00, 98.5MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?export=download&id=1e0JBRmo98zKK8Lbfof9QFZ7IlQiYtDC2\nTo: /kaggle/working/asr_decoded_test.csv\n100%|██████████| 356k/356k [00:00<00:00, 97.8MB/s]\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"'asr_decoded_test.csv'"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"import pandas as pd\nimport torch\n\ndef update_dataset_with_asr(dataset, asr_csv_path, tokenizer, src_lang=\"en_XX\"):\n    # Load the ASR CSV\n    asr_df = pd.read_csv(asr_csv_path)\n    filename_to_asr = dict(zip(asr_df[\"filename\"], asr_df[\"asr_decoded\"]))\n    \n    # Loop through the dataset and modify the fields\n    updated_entries = []\n    \n    for entry in dataset:\n        filename = entry[\"filename\"]  # Get filename from the dataset\n        \n        if filename in filename_to_asr:\n            # Get the corresponding ASR decoded text from CSV\n            asr_text = filename_to_asr[filename]\n            \n            # Tokenize the new ASR text\n            enc = tokenizer(asr_text, return_tensors=\"pt\", padding=False, truncation=True)\n            enc.input_ids[0][0] = tokenizer.lang_code_to_id[src_lang]  # Add language code to first token\n            \n            # Update the dataset entry\n            entry[\"input_ids\"] = enc.input_ids.squeeze(0)\n            entry[\"attention_mask\"] = enc.attention_mask.squeeze(0)\n            entry[\"src\"] = asr_text\n            \n            updated_entries.append(entry)\n        else:\n            # If no match is found, keep the original entry\n            updated_entries.append(entry)\n    \n    return updated_entries\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T11:34:05.888496Z","iopub.execute_input":"2025-05-05T11:34:05.888695Z","iopub.status.idle":"2025-05-05T11:34:05.895031Z","shell.execute_reply.started":"2025-05-05T11:34:05.888678Z","shell.execute_reply":"2025-05-05T11:34:05.894160Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"dev_asr_dataset = update_dataset_with_asr(dev_asr_dataset, \"asr_decoded_dev.csv\", tokenizer)\ntest_asr_dataset = update_dataset_with_asr(test_asr_dataset, \"asr_decoded_test.csv\", tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T11:34:05.895761Z","iopub.execute_input":"2025-05-05T11:34:05.895973Z","iopub.status.idle":"2025-05-05T11:34:15.031676Z","shell.execute_reply.started":"2025-05-05T11:34:05.895958Z","shell.execute_reply":"2025-05-05T11:34:15.031059Z"}},"outputs":[{"name":"stderr","text":"Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"print(len(dev_asr_dataset))\nprint(len(test_asr_dataset))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T11:34:15.033288Z","iopub.execute_input":"2025-05-05T11:34:15.033506Z","iopub.status.idle":"2025-05-05T11:34:15.037564Z","shell.execute_reply.started":"2025-05-05T11:34:15.033489Z","shell.execute_reply":"2025-05-05T11:34:15.036840Z"}},"outputs":[{"name":"stdout","text":"365\n554\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"dev_asr_dataloader = DataLoader(dev_asr_dataset, batch_size=1, shuffle=False)\ntest_asr_dataloader = DataLoader(test_asr_dataset, batch_size=1, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T11:34:15.038186Z","iopub.execute_input":"2025-05-05T11:34:15.038434Z","iopub.status.idle":"2025-05-05T11:34:15.052757Z","shell.execute_reply.started":"2025-05-05T11:34:15.038419Z","shell.execute_reply":"2025-05-05T11:34:15.052133Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T11:34:15.053456Z","iopub.execute_input":"2025-05-05T11:34:15.053721Z","iopub.status.idle":"2025-05-05T11:34:15.079404Z","shell.execute_reply.started":"2025-05-05T11:34:15.053698Z","shell.execute_reply":"2025-05-05T11:34:15.078749Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"MBartForConditionalGeneration(\n  (model): MBartModel(\n    (shared): MBartScaledWordEmbedding(250054, 1024, padding_idx=1)\n    (encoder): MBartEncoder(\n      (embed_tokens): MBartScaledWordEmbedding(250054, 1024, padding_idx=1)\n      (embed_positions): MBartLearnedPositionalEmbedding(1026, 1024)\n      (layers): ModuleList(\n        (0-11): 12 x MBartEncoderLayer(\n          (self_attn): MBartSdpaAttention(\n            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (activation_fn): ReLU()\n          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n    )\n    (decoder): MBartDecoder(\n      (embed_tokens): MBartScaledWordEmbedding(250054, 1024, padding_idx=1)\n      (embed_positions): MBartLearnedPositionalEmbedding(1026, 1024)\n      (layers): ModuleList(\n        (0-11): 12 x MBartDecoderLayer(\n          (self_attn): MBartSdpaAttention(\n            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          )\n          (activation_fn): ReLU()\n          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (encoder_attn): MBartSdpaAttention(\n            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          )\n          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n    )\n  )\n  (lm_head): Linear(in_features=1024, out_features=250054, bias=False)\n)"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\nimport torch\nimport pandas as pd\nimport csv\n\n# Assuming model and tokenizer are already loaded\n# model = MBartForConditionalGeneration.from_pretrained(model_path)\n# tokenizer = MBart50TokenizerFast.from_pretrained(model_path)\n\nmodel.eval()\n\n# Define the source and target languages\nSRC_LANG = \"en_XX\"\nTGT_LANG = \"te_IN\"\n\n# Set the tokenizer source language\ntokenizer.src_lang = SRC_LANG\n\n# Placeholder for storing translations\ntranslations = []\n\n# Prepare CSV file (write header once)\noutput_csv = \"text_dev_asr_translations.csv\"\nwith open(output_csv, mode='w', newline='', encoding='utf-8') as f:\n    writer = csv.DictWriter(f, fieldnames=[\"filename\", \"id\", \"src\", \"tgt\", \"translated\"])\n    writer.writeheader()\n\n    skip2=0\n    # Iterate over the dev_loader\n    for batch in tqdm(dev_asr_dataloader, desc=\"Processing test data\", unit=\"batch\"):\n        # Assuming 'input_ids' and 'attention_mask' are present in the batch\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        punct_feats = batch['punct_feats'].to(device)\n        \n        # Unwrap values from lists or tensors\n        filename = batch['filename'][0] if isinstance(batch['filename'], list) else batch['filename']\n        fileid = batch['id'].item() if isinstance(batch['id'], torch.Tensor) else batch['id']\n        src_text = batch['src'][0] if isinstance(batch['src'], list) else batch['src']\n        tgt_text = batch['tgt'][0] if isinstance(batch['tgt'], list) else batch['tgt']\n\n        \n        # Set the language token for the source\n        input_ids[0][0] = tokenizer.lang_code_to_id[SRC_LANG]\n        \n        # Run the encoder manually\n        with torch.no_grad():\n            encoder_outputs = model.model.encoder(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                return_dict=True\n            )\n    \n        try:\n            # Add punctuation features to encoder hidden states\n            combined_hidden_state = encoder_outputs.last_hidden_state\n        except:\n            skip2+=1\n            # print('skipping :(')\n            continue\n    \n        # Wrap into BaseModelOutput\n        encoder_outputs = BaseModelOutput(last_hidden_state=combined_hidden_state)\n    \n        # Generate translation from decoder with forced beginning token for Telugu\n        generated_tokens = model.generate(\n            input_ids=None,\n            encoder_outputs=encoder_outputs,\n            attention_mask=None,\n            forced_bos_token_id=tokenizer.lang_code_to_id[TGT_LANG],\n            max_length=50\n        )\n        \n        # Decode the generated tokens into text\n        translated_text = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n        #print(translated_text)\n        translations.append(translated_text)\n    \n        writer.writerow({\n                    \"filename\": filename,\n                    \"id\": fileid,\n                    \"src\": src_text,\n                    \"tgt\": tgt_text,\n                    \"translated\": translated_text\n                })\n    \n    \n    # Print out translations for the entire dev set\n    for translated_text in translations:\n        print(translated_text)\n        break\n    \n    print(skip2)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df= pd.read_csv(\"text_dev_asr_translations.csv\")\nprint(df.head())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\nimport torch\nimport pandas as pd\nimport csv\n\nmodel.eval()\n\n# Define the source and target languages\nSRC_LANG = \"en_XX\"\nTGT_LANG = \"te_IN\"\n\n# Set the tokenizer source language\ntokenizer.src_lang = SRC_LANG\n\n# Placeholder for storing translations\ntranslations = []\n\n# Prepare CSV file (write header once)\noutput_csv = \"text_test_asr_translations_3.csv\"\nwith open(output_csv, mode='w', newline='', encoding='utf-8') as f:\n    writer = csv.DictWriter(f, fieldnames=[\"filename\", \"id\", \"src\", \"tgt\", \"translated\"])\n    writer.writeheader()\n\n    skip2=0\n    # Iterate over the dev_loader\n    for batch in tqdm(test_asr_dataloader, desc=\"Processing test data\", unit=\"batch\"):\n        # Assuming 'input_ids' and 'attention_mask' are present in the batch\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        punct_feats = batch['punct_feats'].to(device)\n        \n        # Unwrap values from lists or tensors\n        filename = batch['filename'][0] if isinstance(batch['filename'], list) else batch['filename']\n        fileid = batch['id'].item() if isinstance(batch['id'], torch.Tensor) else batch['id']\n        src_text = batch['src'][0] if isinstance(batch['src'], list) else batch['src']\n        tgt_text = batch['tgt'][0] if isinstance(batch['tgt'], list) else batch['tgt']\n\n        \n        # Set the language token for the source\n        input_ids[0][0] = tokenizer.lang_code_to_id[SRC_LANG]\n        \n        # Run the encoder manually\n        with torch.no_grad():\n            encoder_outputs = model.model.encoder(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                return_dict=True\n            )\n    \n        try:\n            # Add punctuation features to encoder hidden states\n            combined_hidden_state = encoder_outputs.last_hidden_state\n        except:\n            skip2+=1\n            # print('skipping :(')\n            continue\n    \n        # Wrap into BaseModelOutput\n        encoder_outputs = BaseModelOutput(last_hidden_state=combined_hidden_state)\n    \n        # Generate translation from decoder with forced beginning token for Telugu\n        generated_tokens = model.generate(\n            input_ids=None,\n            encoder_outputs=encoder_outputs,\n            attention_mask=None,\n            forced_bos_token_id=tokenizer.lang_code_to_id[TGT_LANG],\n            max_length=50\n        )\n        \n        # Decode the generated tokens into text\n        translated_text = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n        translations.append(translated_text)\n    \n        writer.writerow({\n                    \"filename\": filename,\n                    \"id\": fileid,\n                    \"src\": src_text,\n                    \"tgt\": tgt_text,\n                    \"translated\": translated_text\n                })\n    \n    \n    # Print out translations for the entire dev set\n    for translated_text in translations:\n        print(translated_text)\n        break\n    \n    print(skip2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T11:34:48.869174Z","iopub.execute_input":"2025-05-05T11:34:48.870057Z","iopub.status.idle":"2025-05-05T11:41:27.880835Z","shell.execute_reply.started":"2025-05-05T11:34:48.870024Z","shell.execute_reply":"2025-05-05T11:41:27.880117Z"}},"outputs":[{"name":"stderr","text":"Processing test data: 100%|██████████| 554/554 [06:38<00:00,  1.39batch/s]","output_type":"stream"},{"name":"stdout","text":"నాగరికత పదం లాటిస్ సైవిస్ కు చెందినది, అంటే నాగరికత, అంటే పౌరునికి చెందినది, అంటే పట్టణం లేదా నగర-రాజ్యకి చెందినది. ఇంకా ఇది ఒక కంపెనీ యొక్క పరిమా\n0\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"df= pd.read_csv(\"text_test_asr_translations_2.csv\")\nprint(df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T11:41:31.846191Z","iopub.execute_input":"2025-05-05T11:41:31.846535Z","iopub.status.idle":"2025-05-05T11:41:31.876610Z","shell.execute_reply.started":"2025-05-05T11:41:31.846509Z","shell.execute_reply":"2025-05-05T11:41:31.875767Z"}},"outputs":[{"name":"stdout","text":"                   filename    id  \\\n0   7266355212113564962.wav  1892   \n1   5381110210154713971.wav  1902   \n2   1724953769276277810.wav  1944   \n3   4705502721902980056.wav  1839   \n4  17498257810809617374.wav  1937   \n\n                                                 src  \\\n0  This is the place the British colonisers took ...   \n1  He was initially hospitalised in the James Pag...   \n2  These are sometimes-crowded family beaches wit...   \n3  Think of the skiing route as of a similar hiki...   \n4  However, these plans were rendered obsolete ne...   \n\n                                                 tgt  \\\n0  బ్రిటిష్ వలసవాదులు తమ సొంత ప్రదేశంగా తీసుకున్న...   \n1  అతను ప్రారంభంలో గ్రేట్ యార్మౌత్ లోని జేమ్స్ పే...   \n2  ఇవి కొన్నిసార్లు కుటుంబాలతో రద్దీగా ఉండే బీచ్‌...   \n3  అదే విధమైన హైకింగ్ రూట్ వలే స్కీయింగ్ రూట్ గుర...   \n4  ఏది ఏమయినప్పటికీ, రాత్రికి రాత్రే పథకాన్ని అమల...   \n\n                                          translated  \n0  బ్రిటిష్ కాలనీడర్లు తమ సొంతంగా తీసుకున్న స్థలం...  \n1     అతను మొదట జేమ్స్ పేయింట్ హాస్పిటల్ లో ఉన్నాడు.  \n2  ఈ కొన్నిసార్లు-సమూహంలో ఉన్న కుటుంబ బీచ్ లు, వీ...  \n3  స్కీయింగ్ మార్గంని ఇలాంటి హైకింగ్ మార్గంగా ఆలో...  \n4  అయితే, ఈ ప్రణాళికలు దాదాపు రాత్రిపూట మసివాట్ య...  \n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import gdown\n\n#train\ncsv_file_id = '1-DfbuRqP5xe9eNMjobEDH_cZR1FlwyEw'\ngdown.download(f'https://drive.google.com/uc?export=download&id={csv_file_id}', 'MT_train.csv', quiet=True)\ntsv_file_id = '1pBYaqKbkJt66bJlBJGKvTaWVI6Q0yNCC'\ngdown.download(f'https://drive.google.com/uc?export=download&id={tsv_file_id}', 'train.tsv', quiet=True)\nzip_file_id = '1BkvxK5q0D7QUGuf7GkXY14inDaYE5HbN'\ngdown.download(f'https://drive.google.com/uc?export=download&id={zip_file_id}', 'punctuation_train.zip', quiet=True)\n\n#dev\ncsv_file_id = '1-I-VFAdiAvFF-7x_Y_0_TwDf0v2kl80l'\ngdown.download(f'https://drive.google.com/uc?export=download&id={csv_file_id}', 'MT_dev.csv', quiet=True)\ntsv_file_id = '1M6uGyGSOEW7wKirzi7dlizlRaN9wStwD'\ngdown.download(f'https://drive.google.com/uc?export=download&id={tsv_file_id}', 'dev.tsv', quiet=True)\nzip_file_id = '1gabCrZ7iAvEDBCflW1lvxgLc0ZztWSb_'\ngdown.download(f'https://drive.google.com/uc?export=download&id={zip_file_id}', 'punctuation_dev.zip', quiet=True)\n\n#test\ncsv_file_id = '1-GVAKGJs92uzo8bF1d-q9J_apyqP1ofG'\ngdown.download(f'https://drive.google.com/uc?export=download&id={csv_file_id}', 'MT_test.csv', quiet=True)\ntsv_file_id = '1m7JIcj_-SVYASbcbBiT4LoPdgZdqGN4i'\ngdown.download(f'https://drive.google.com/uc?export=download&id={tsv_file_id}', 'test.tsv', quiet=True)\nzip_file_id = '1Nju6_FKSj3X-PVAG3WVAoX3v-o3jJQWL'\ngdown.download(f'https://drive.google.com/uc?export=download&id={zip_file_id}', 'punctuation_test.zip', quiet=True)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T09:25:54.276871Z","iopub.execute_input":"2025-05-05T09:25:54.277130Z","iopub.status.idle":"2025-05-05T09:26:36.385544Z","shell.execute_reply.started":"2025-05-05T09:25:54.277109Z","shell.execute_reply":"2025-05-05T09:26:36.384948Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"'punctuation_test.zip'"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"import os\nimport zipfile\nimport torch\nimport pandas as pd\nfrom transformers import MBartTokenizer\n\nclass EnglishTeluguPunctDataset:\n    def __init__(self, zip_path, tsv_path, csv_path, tokenizer, max_length=128):\n        self.zip_path = zip_path\n        self.tsv_path = tsv_path\n        self.csv_path = csv_path\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.skipped = 0\n\n        # Load MT_data.csv (contains both English and Telugu)\n        mt_data = pd.read_csv(csv_path)\n        self.id_to_english = mt_data.set_index(\"id\")[\"english\"].to_dict()\n        self.id_to_telugu = mt_data.set_index(\"id\")[\"telugu\"].to_dict()\n\n        # Load eng_data.tsv (only for filename to ID mapping)\n        eng_tsv = pd.read_csv(tsv_path, sep=\"\\t\", header=None, dtype=str)\n        eng_tsv = eng_tsv[eng_tsv.columns[:2]]\n        eng_tsv.columns = [\"id\", \"filename\"]\n        eng_tsv[\"id\"] = eng_tsv[\"id\"].astype(int)\n        self.filename_to_id = eng_tsv.set_index(\"filename\")[\"id\"].to_dict()\n\n        # Read .pt filenames from ZIP\n        with zipfile.ZipFile(zip_path, 'r') as zipf:\n            self.pt_filenames = [f for f in zipf.namelist() if f.endswith('.pt')]\n\n        # Match each .pt file to a dataset entry\n        self.valid_entries = []\n        for pt_file in self.pt_filenames:\n            base = os.path.basename(pt_file)\n            filename_without_extension = base.replace(\".pt\", \"\")\n\n            if filename_without_extension in self.filename_to_id:\n                id_ = self.filename_to_id[filename_without_extension]\n                if id_ in self.id_to_english and id_ in self.id_to_telugu:\n                    self.valid_entries.append((pt_file, id_))\n                else:\n                    self.skipped += 1\n            else:\n                self.skipped += 1\n\n    def __len__(self):\n        return len(self.valid_entries)\n\n    def __getitem__(self, idx):\n        pt_file, id_ = self.valid_entries[idx]\n    \n        # Load punctuation features from .pt file\n        with zipfile.ZipFile(self.zip_path, 'r') as zipf:\n            with zipf.open(pt_file) as f:\n                punct_feats = torch.load(f, weights_only=True)\n    \n        # Get English and Telugu text\n        src_text = self.id_to_english[id_]\n        tgt_text = self.id_to_telugu[id_]\n    \n        # Tokenize without max_length padding\n        src_enc = self.tokenizer(src_text, return_tensors=\"pt\", padding=False, truncation=True)\n        tgt_enc = self.tokenizer(tgt_text, return_tensors=\"pt\", padding=False, truncation=True)\n    \n        # Set language codes as first token\n        src_lang = \"en_XX\"\n        tgt_lang = \"te_IN\"\n        src_enc.input_ids[0][0] = self.tokenizer.lang_code_to_id[src_lang]\n        tgt_enc.input_ids[0][0] = self.tokenizer.lang_code_to_id[tgt_lang]\n    \n        return {\n            \"input_ids\": src_enc.input_ids.squeeze(0),\n            \"attention_mask\": src_enc.attention_mask.squeeze(0),\n            \"labels\": tgt_enc.input_ids.squeeze(0),\n            \"punct_feats\": punct_feats,\n            \"src\": src_text,\n            \"tgt\": tgt_text,\n            \"filename\": os.path.basename(pt_file).replace(\".pt\", \"\"),\n            \"id\": id_\n        }\n\n    def get_skipped_count(self):\n        return self.skipped\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T09:27:58.292971Z","iopub.execute_input":"2025-05-05T09:27:58.293626Z","iopub.status.idle":"2025-05-05T09:28:04.994971Z","shell.execute_reply.started":"2025-05-05T09:27:58.293592Z","shell.execute_reply":"2025-05-05T09:28:04.994180Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# model initialization","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom transformers import MBartForConditionalGeneration, MBart50TokenizerFast\nfrom torch.optim import AdamW\nfrom tqdm import tqdm\nfrom transformers.modeling_outputs import BaseModelOutput\nfrom torch.utils.data import DataLoader\n\n\n\nmodel_name = \"facebook/mbart-large-50-many-to-many-mmt\"\n# Load tokenizer and model\n# tokenizer = MBart50TokenizerFast.from_pretrained(model_name)\nmodel = MBartForConditionalGeneration.from_pretrained(model_name)\n\n# Initialize the tokenizer\ntokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T09:28:04.996081Z","iopub.execute_input":"2025-05-05T09:28:04.996729Z","iopub.status.idle":"2025-05-05T09:28:38.627854Z","shell.execute_reply.started":"2025-05-05T09:28:04.996708Z","shell.execute_reply":"2025-05-05T09:28:38.627259Z"}},"outputs":[{"name":"stderr","text":"2025-05-05 09:28:12.145942: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1746437292.377525      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1746437292.445946      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.43k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56c4491e79f8446589d2b7496e45a834"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.44G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"146437f44b3b4ab498bd11a08ce25946"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/261 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9998a244e404471dac18aab3f26435ec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/529 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ffd9fdfef94c478e95610bb5357d3b53"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f075b681fd4241a5957e559e13b2a0a4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/649 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"669cbd3ada6540e4bd25b64a106ad0f6"}},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"# dataloaders","metadata":{}},{"cell_type":"code","source":"\n# Initialize the dataset\ntrain_dataset = EnglishTeluguPunctDataset(zip_path=\"punctuation_train.zip\",\n                                    tsv_path=\"train.tsv\",\n                                    csv_path=\"MT_train.csv\",\n                                    tokenizer=tokenizer)\n# Create DataLoader\ntrain_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n\n# Initialize the dataset\ndev_dataset = EnglishTeluguPunctDataset(zip_path=\"punctuation_dev.zip\",\n                                    tsv_path=\"dev.tsv\",\n                                    csv_path=\"MT_dev.csv\",\n                                    tokenizer=tokenizer)\n\n# Create DataLoader\ndev_dataloader = DataLoader(dev_dataset, batch_size=1, shuffle=True)\n\n\n\n# Initialize the dataset\ntest_dataset = EnglishTeluguPunctDataset(zip_path=\"punctuation_test.zip\",\n                                    tsv_path=\"test.tsv\",\n                                    csv_path=\"MT_test.csv\",\n                                    tokenizer=tokenizer)\n\n# Create DataLoader\ntest_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T17:19:55.844473Z","iopub.execute_input":"2025-05-04T17:19:55.845103Z","iopub.status.idle":"2025-05-04T17:19:55.937515Z","shell.execute_reply.started":"2025-05-04T17:19:55.845079Z","shell.execute_reply":"2025-05-04T17:19:55.936561Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(len(train_dataset))\nprint(len(dev_dataset))\nprint(len(test_dataset))\nprint(len(dev_asr_dataset))\nprint(len(test_asr_dataset))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T17:19:55.938503Z","iopub.execute_input":"2025-05-04T17:19:55.938787Z","iopub.status.idle":"2025-05-04T17:19:55.943624Z","shell.execute_reply.started":"2025-05-04T17:19:55.938768Z","shell.execute_reply":"2025-05-04T17:19:55.942769Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(len(train_dataloader))\nprint(len(dev_dataloader))\nprint(len(test_dataloader))\nprint(len(dev_asr_dataloader))\nprint(len(test_asr_dataloader))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T17:19:55.944522Z","iopub.execute_input":"2025-05-04T17:19:55.945273Z","iopub.status.idle":"2025-05-04T17:19:55.960070Z","shell.execute_reply.started":"2025-05-04T17:19:55.945239Z","shell.execute_reply":"2025-05-04T17:19:55.959310Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# training ","metadata":{}},{"cell_type":"code","source":"SRC_LANG = \"en_XX\"\nTGT_LANG = \"te_IN\"\n\ntokenizer.src_lang = SRC_LANG\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Set model to training mode\nmodel.train()\n\noptimizer = AdamW(model.parameters(), lr=1e-5)\n\nimport torch.nn as nn\n\nfrom transformers.modeling_outputs import BaseModelOutput\nskip=0\n\nimport torch.nn as nn\n\n# Assume punct_feats shape: (batch_size, seq_len, feat_dim)\nfeat_dim = train_dataset[0][\"punct_feats\"].shape[1]\nhidden_dim = model.model.encoder.config.d_model  # mBART hidden size (e.g., 1024)\n\n# Punctuation feature encoder\npunct_feat_encoder = nn.Linear(feat_dim, hidden_dim).to(device)\n\n# Add to optimizer\noptimizer = AdamW(list(model.parameters()) + list(punct_feat_encoder.parameters()), lr=5e-5)\n\n\nfor epoch in range(3):\n    model.train()\n    punct_feat_encoder.train()\n\n    for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}\"):\n        optimizer.zero_grad()\n\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n        punct_feats = batch['punct_feats'].to(device)  # (batch, seq_len, feat_dim)\n\n        # Forward encoder\n        encoder_outputs = model.model.encoder(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            return_dict=True\n        )\n\n        # Pass punctuation features through its encoder\n        try:\n            encoded_punct_feats = punct_feat_encoder(punct_feats)  # (batch, seq_len, hidden_dim)\n            combined_hidden_state = encoder_outputs.last_hidden_state + encoded_punct_feats\n        except Exception as e:\n            skip += 1\n            continue\n\n        encoder_outputs = BaseModelOutput(last_hidden_state=combined_hidden_state)\n        decoder_input_ids = model.prepare_decoder_input_ids_from_labels(labels)\n\n        outputs = model(\n            input_ids=None,\n            attention_mask=None,\n            decoder_input_ids=decoder_input_ids,\n            encoder_outputs=encoder_outputs,\n            labels=labels,\n            return_dict=True\n        )\n\n        loss = outputs.loss\n        if torch.isnan(loss):\n            continue\n        loss.backward()\n        optimizer.step()\n\n    print(f\"Epoch {epoch + 1}, Loss: {loss.item():.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T17:26:50.737623Z","iopub.execute_input":"2025-05-04T17:26:50.738200Z","iopub.status.idle":"2025-05-04T17:38:03.714669Z","shell.execute_reply.started":"2025-05-04T17:26:50.738175Z","shell.execute_reply":"2025-05-04T17:38:03.714009Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.save(model.state_dict(), \"mbart_en_te_both_punct_final.pt\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T17:40:55.173026Z","iopub.execute_input":"2025-05-04T17:40:55.173699Z","iopub.status.idle":"2025-05-04T17:40:59.116708Z","shell.execute_reply.started":"2025-05-04T17:40:55.173673Z","shell.execute_reply":"2025-05-04T17:40:59.116105Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import FileLink\n\n# Replace with your actual .pt file name\nFileLink('mbart_en_te_both_punct_final.pt')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T17:52:57.692947Z","iopub.execute_input":"2025-05-04T17:52:57.693549Z","iopub.status.idle":"2025-05-04T17:52:57.698800Z","shell.execute_reply.started":"2025-05-04T17:52:57.693521Z","shell.execute_reply":"2025-05-04T17:52:57.697987Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import zipfile\n\n# Path to the .pt file\npt_file_path = 'mbart_en_te_both_punct_final.pt'\n\n# Name of the zip file to create\nzip_file_path = 'mbart_en_te_both_punct_final.zip'\n\n# Add .pt file to zip\nwith zipfile.ZipFile(zip_file_path, 'w') as zipf:\n    zipf.write(pt_file_path, arcname='mbart_en_te_both_punct_final.pt')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T17:47:16.388929Z","iopub.execute_input":"2025-05-04T17:47:16.389895Z","iopub.status.idle":"2025-05-04T17:47:22.005534Z","shell.execute_reply.started":"2025-05-04T17:47:16.389867Z","shell.execute_reply":"2025-05-04T17:47:22.004889Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_id = '1GAerGIPfUzEi-ebDayqBGgQa7h7xi2Y0'\ngdown.download(f'https://drive.google.com/uc?export=download&id={model_id}', 'both.pt', quiet=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T09:28:50.594081Z","iopub.execute_input":"2025-05-05T09:28:50.594870Z","iopub.status.idle":"2025-05-05T09:29:20.941109Z","shell.execute_reply.started":"2025-05-05T09:28:50.594842Z","shell.execute_reply":"2025-05-05T09:29:20.940515Z"}},"outputs":[{"name":"stderr","text":"Downloading...\nFrom (original): https://drive.google.com/uc?export=download&id=1GAerGIPfUzEi-ebDayqBGgQa7h7xi2Y0\nFrom (redirected): https://drive.google.com/uc?export=download&id=1GAerGIPfUzEi-ebDayqBGgQa7h7xi2Y0&confirm=t&uuid=f6a2263f-2437-48d4-8507-534306f5cd28\nTo: /kaggle/working/both.pt\n100%|██████████| 2.44G/2.44G [00:26<00:00, 92.9MB/s]\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"'both.pt'"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"state_dict = torch.load('both.pt', weights_only=True)\nmodel.load_state_dict(state_dict)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T09:29:20.942228Z","iopub.execute_input":"2025-05-05T09:29:20.943003Z","iopub.status.idle":"2025-05-05T09:29:26.364865Z","shell.execute_reply.started":"2025-05-05T09:29:20.942977Z","shell.execute_reply":"2025-05-05T09:29:26.364043Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\nimport torch\nimport pandas as pd\nimport csv\n\nmodel.eval()\n\n# Define the source and target languages\nSRC_LANG = \"en_XX\"\nTGT_LANG = \"te_IN\"\n\n# Set the tokenizer source language\ntokenizer.src_lang = SRC_LANG\n\n# Placeholder for storing translations\ntranslations = []\n\n# Prepare CSV file (write header once)\noutput_csv = \"both_dev_translations.csv\"\nwith open(output_csv, mode='w', newline='', encoding='utf-8') as f:\n    writer = csv.DictWriter(f, fieldnames=[\"filename\", \"id\", \"src\", \"tgt\", \"translated\"])\n    writer.writeheader()\n\n    skip2=0\n    # Iterate over the dev_loader\n    for batch in tqdm(dev_dataloader, desc=\"Processing test data\", unit=\"batch\"):\n        # Assuming 'input_ids' and 'attention_mask' are present in the batch\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        punct_feats = batch['punct_feats'].to(device)\n        \n        # Unwrap values from lists or tensors\n        filename = batch['filename'][0] if isinstance(batch['filename'], list) else batch['filename']\n        fileid = batch['id'].item() if isinstance(batch['id'], torch.Tensor) else batch['id']\n        src_text = batch['src'][0] if isinstance(batch['src'], list) else batch['src']\n        tgt_text = batch['tgt'][0] if isinstance(batch['tgt'], list) else batch['tgt']\n\n        \n        # Set the language token for the source\n        input_ids[0][0] = tokenizer.lang_code_to_id[SRC_LANG]\n        \n        # Run the encoder manually\n        with torch.no_grad():\n            encoder_outputs = model.model.encoder(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                return_dict=True\n            )\n    \n        try:\n            # Add punctuation features to encoder hidden states\n            combined_hidden_state = encoder_outputs.last_hidden_state\n        except:\n            skip2+=1\n            # print('skipping :(')\n            continue\n    \n        # Wrap into BaseModelOutput\n        encoder_outputs = BaseModelOutput(last_hidden_state=combined_hidden_state)\n    \n        # Generate translation from decoder with forced beginning token for Telugu\n        generated_tokens = model.generate(\n            input_ids=None,\n            encoder_outputs=encoder_outputs,\n            attention_mask=None,\n            forced_bos_token_id=tokenizer.lang_code_to_id[TGT_LANG],\n            max_length=50\n        )\n        \n        # Decode the generated tokens into text\n        translated_text = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n        #print(translated_text)\n        translations.append(translated_text)\n    \n        writer.writerow({\n                    \"filename\": filename,\n                    \"id\": fileid,\n                    \"src\": src_text,\n                    \"tgt\": tgt_text,\n                    \"translated\": translated_text\n                })\n    \n    \n    # Print out translations for the entire dev set\n    for translated_text in translations:\n        print(translated_text)\n        break\n    \n    print(skip2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T17:39:16.716836Z","iopub.execute_input":"2025-05-04T17:39:16.717451Z","iopub.status.idle":"2025-05-04T17:40:34.718328Z","shell.execute_reply.started":"2025-05-04T17:39:16.717426Z","shell.execute_reply":"2025-05-04T17:40:34.717714Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df= pd.read_csv(\"both_dev_translations.csv\")\nprint(df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T17:41:02.076052Z","iopub.execute_input":"2025-05-04T17:41:02.076316Z","iopub.status.idle":"2025-05-04T17:41:02.093157Z","shell.execute_reply.started":"2025-05-04T17:41:02.076299Z","shell.execute_reply":"2025-05-04T17:41:02.092192Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\nimport torch\nimport pandas as pd\nimport csv\n\n# Assuming model and tokenizer are already loaded\n# model = MBartForConditionalGeneration.from_pretrained(model_path)\n# tokenizer = MBart50TokenizerFast.from_pretrained(model_path)\n\nmodel.eval()\n\n# Define the source and target languages\nSRC_LANG = \"en_XX\"\nTGT_LANG = \"te_IN\"\n\n# Set the tokenizer source language\ntokenizer.src_lang = SRC_LANG\n\n# Placeholder for storing translations\ntranslations = []\n\n# Prepare CSV file (write header once)\noutput_csv = \"both_test_translations.csv\"\nwith open(output_csv, mode='w', newline='', encoding='utf-8') as f:\n    writer = csv.DictWriter(f, fieldnames=[\"filename\", \"id\", \"src\", \"tgt\", \"translated\"])\n    writer.writeheader()\n\n    skip2=0\n    # Iterate over the dev_loader\n    for batch in tqdm(test_dataloader, desc=\"Processing test data\", unit=\"batch\"):\n        # Assuming 'input_ids' and 'attention_mask' are present in the batch\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        punct_feats = batch['punct_feats'].to(device)\n        \n        # Unwrap values from lists or tensors\n        filename = batch['filename'][0] if isinstance(batch['filename'], list) else batch['filename']\n        fileid = batch['id'].item() if isinstance(batch['id'], torch.Tensor) else batch['id']\n        src_text = batch['src'][0] if isinstance(batch['src'], list) else batch['src']\n        tgt_text = batch['tgt'][0] if isinstance(batch['tgt'], list) else batch['tgt']\n\n        \n        # Set the language token for the source\n        input_ids[0][0] = tokenizer.lang_code_to_id[SRC_LANG]\n        \n        # Run the encoder manually\n        with torch.no_grad():\n            encoder_outputs = model.model.encoder(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                return_dict=True\n            )\n    \n        try:\n            # Add punctuation features to encoder hidden states\n            combined_hidden_state = encoder_outputs.last_hidden_state\n        except:\n            skip2+=1\n            # print('skipping :(')\n            continue\n    \n        # Wrap into BaseModelOutput\n        encoder_outputs = BaseModelOutput(last_hidden_state=combined_hidden_state)\n    \n        # Generate translation from decoder with forced beginning token for Telugu\n        generated_tokens = model.generate(\n            input_ids=None,\n            encoder_outputs=encoder_outputs,\n            attention_mask=None,\n            forced_bos_token_id=tokenizer.lang_code_to_id[TGT_LANG],\n            max_length=50\n        )\n        \n        # Decode the generated tokens into text\n        translated_text = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n        translations.append(translated_text)\n    \n        writer.writerow({\n                    \"filename\": filename,\n                    \"id\": fileid,\n                    \"src\": src_text,\n                    \"tgt\": tgt_text,\n                    \"translated\": translated_text\n                })\n    \n    \n    # Print out translations for the entire dev set\n    for translated_text in translations:\n        print(translated_text)\n        break\n    \n    print(skip2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T17:41:19.873857Z","iopub.execute_input":"2025-05-04T17:41:19.874512Z","iopub.status.idle":"2025-05-04T17:43:13.344765Z","shell.execute_reply.started":"2025-05-04T17:41:19.874483Z","shell.execute_reply":"2025-05-04T17:43:13.343989Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df= pd.read_csv(\"both_test_translations.csv\")\nprint(df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T17:43:13.345875Z","iopub.execute_input":"2025-05-04T17:43:13.346126Z","iopub.status.idle":"2025-05-04T17:43:13.356745Z","shell.execute_reply.started":"2025-05-04T17:43:13.346099Z","shell.execute_reply":"2025-05-04T17:43:13.356101Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"zip_file_id = '1R6QIiCdBHKtvz7oi7xXCDbGFIBm2YfIZ'\ngdown.download(f'https://drive.google.com/uc?export=download&id={zip_file_id}', 'punctuation_dev_asr.zip', quiet=False)\n\nzip_file_id = '1RO8dYAIzrCCAJ-ST5UJJTtetumY5XRRO'\ngdown.download(f'https://drive.google.com/uc?export=download&id={zip_file_id}', 'punctuation_test_asr.zip', quiet=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T09:30:04.177797Z","iopub.execute_input":"2025-05-05T09:30:04.178342Z","iopub.status.idle":"2025-05-05T09:30:14.083202Z","shell.execute_reply.started":"2025-05-05T09:30:04.178321Z","shell.execute_reply":"2025-05-05T09:30:14.082502Z"}},"outputs":[{"name":"stderr","text":"Downloading...\nFrom: https://drive.google.com/uc?export=download&id=1R6QIiCdBHKtvz7oi7xXCDbGFIBm2YfIZ\nTo: /kaggle/working/punctuation_dev_asr.zip\n100%|██████████| 6.79M/6.79M [00:00<00:00, 56.7MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?export=download&id=1RO8dYAIzrCCAJ-ST5UJJTtetumY5XRRO\nTo: /kaggle/working/punctuation_test_asr.zip\n100%|██████████| 10.0M/10.0M [00:00<00:00, 93.9MB/s]\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"'punctuation_test_asr.zip'"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"# Initialize the dataset\ndev_asr_dataset = EnglishTeluguPunctDataset(zip_path=\"punctuation_dev_asr.zip\",\n                                    tsv_path=\"dev.tsv\",\n                                    csv_path=\"MT_dev.csv\",\n                                    tokenizer=tokenizer)\n\n# Initialize the dataset\ntest_asr_dataset = EnglishTeluguPunctDataset(zip_path=\"punctuation_test_asr.zip\",\n                                    tsv_path=\"test.tsv\",\n                                    csv_path=\"MT_test.csv\",\n                                    tokenizer=tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T09:31:03.975659Z","iopub.execute_input":"2025-05-05T09:31:03.976206Z","iopub.status.idle":"2025-05-05T09:31:04.023719Z","shell.execute_reply.started":"2025-05-05T09:31:03.976180Z","shell.execute_reply":"2025-05-05T09:31:04.023170Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"csv_file_id = '1FR-MyJRxZH62grW7DihFqbgA8HG0Wnne'\ngdown.download(f'https://drive.google.com/uc?export=download&id={csv_file_id}', 'asr_decoded_dev.csv', quiet=False)\n\ncsv_file_id = '1e0JBRmo98zKK8Lbfof9QFZ7IlQiYtDC2'\ngdown.download(f'https://drive.google.com/uc?export=download&id={csv_file_id}', 'asr_decoded_test.csv', quiet=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T09:31:04.748303Z","iopub.execute_input":"2025-05-05T09:31:04.748597Z","iopub.status.idle":"2025-05-05T09:31:11.948445Z","shell.execute_reply.started":"2025-05-05T09:31:04.748576Z","shell.execute_reply":"2025-05-05T09:31:11.947918Z"}},"outputs":[{"name":"stderr","text":"Downloading...\nFrom: https://drive.google.com/uc?export=download&id=1FR-MyJRxZH62grW7DihFqbgA8HG0Wnne\nTo: /kaggle/working/asr_decoded_dev.csv\n100%|██████████| 227k/227k [00:00<00:00, 74.3MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?export=download&id=1e0JBRmo98zKK8Lbfof9QFZ7IlQiYtDC2\nTo: /kaggle/working/asr_decoded_test.csv\n100%|██████████| 356k/356k [00:00<00:00, 106MB/s]\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"'asr_decoded_test.csv'"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"import pandas as pd\nimport torch\n\ndef update_dataset_with_asr(dataset, asr_csv_path, tokenizer, src_lang=\"en_XX\"):\n    # Load the ASR CSV\n    asr_df = pd.read_csv(asr_csv_path)\n    filename_to_asr = dict(zip(asr_df[\"filename\"], asr_df[\"asr_decoded\"]))\n    \n    # Loop through the dataset and modify the fields\n    updated_entries = []\n    \n    for entry in dataset:\n        filename = entry[\"filename\"]  # Get filename from the dataset\n        \n        if filename in filename_to_asr:\n            # Get the corresponding ASR decoded text from CSV\n            asr_text = filename_to_asr[filename]\n            \n            # Tokenize the new ASR text\n            enc = tokenizer(asr_text, return_tensors=\"pt\", padding=False, truncation=True)\n            enc.input_ids[0][0] = tokenizer.lang_code_to_id[src_lang]  # Add language code to first token\n            \n            # Update the dataset entry\n            entry[\"input_ids\"] = enc.input_ids.squeeze(0)\n            entry[\"attention_mask\"] = enc.attention_mask.squeeze(0)\n            entry[\"src\"] = asr_text\n            \n            updated_entries.append(entry)\n        else:\n            # If no match is found, keep the original entry\n            updated_entries.append(entry)\n    \n    return updated_entries\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T09:31:11.949415Z","iopub.execute_input":"2025-05-05T09:31:11.949662Z","iopub.status.idle":"2025-05-05T09:31:11.954921Z","shell.execute_reply.started":"2025-05-05T09:31:11.949646Z","shell.execute_reply":"2025-05-05T09:31:11.954250Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"dev_asr_dataset = update_dataset_with_asr(dev_asr_dataset, \"asr_decoded_dev.csv\", tokenizer)\ntest_asr_dataset = update_dataset_with_asr(test_asr_dataset, \"asr_decoded_test.csv\", tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T09:31:11.955665Z","iopub.execute_input":"2025-05-05T09:31:11.955905Z","iopub.status.idle":"2025-05-05T09:31:12.409691Z","shell.execute_reply.started":"2025-05-05T09:31:11.955885Z","shell.execute_reply":"2025-05-05T09:31:12.409154Z"}},"outputs":[{"name":"stderr","text":"Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"print(len(dev_asr_dataset))\nprint(len(test_asr_dataset))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T09:31:12.410794Z","iopub.execute_input":"2025-05-05T09:31:12.411038Z","iopub.status.idle":"2025-05-05T09:31:12.414976Z","shell.execute_reply.started":"2025-05-05T09:31:12.411013Z","shell.execute_reply":"2025-05-05T09:31:12.414331Z"}},"outputs":[{"name":"stdout","text":"61\n85\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"dev_asr_dataloader = DataLoader(dev_asr_dataset, batch_size=1, shuffle=False)\ntest_asr_dataloader = DataLoader(test_asr_dataset, batch_size=1, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T09:31:21.406132Z","iopub.execute_input":"2025-05-05T09:31:21.406936Z","iopub.status.idle":"2025-05-05T09:31:21.411658Z","shell.execute_reply.started":"2025-05-05T09:31:21.406905Z","shell.execute_reply":"2025-05-05T09:31:21.410908Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T09:31:21.792820Z","iopub.execute_input":"2025-05-05T09:31:21.793058Z","iopub.status.idle":"2025-05-05T09:31:21.796651Z","shell.execute_reply.started":"2025-05-05T09:31:21.793041Z","shell.execute_reply":"2025-05-05T09:31:21.795936Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\nimport torch\nimport pandas as pd\nimport csv\n\nmodel.to(device)\nmodel.eval()\n\n# Define the source and target languages\nSRC_LANG = \"en_XX\"\nTGT_LANG = \"te_IN\"\n\n# Set the tokenizer source language\ntokenizer.src_lang = SRC_LANG\n\n# Placeholder for storing translations\ntranslations = []\n\n# Prepare CSV file (write header once)\noutput_csv = \"both_dev_asr_translations_final_returns.csv\"\nwith open(output_csv, mode='w', newline='', encoding='utf-8') as f:\n    writer = csv.DictWriter(f, fieldnames=[\"filename\", \"id\", \"src\", \"tgt\", \"translated\"])\n    writer.writeheader()\n\n    skip2=0\n    # Iterate over the dev_loader\n    for batch in tqdm(dev_asr_dataloader, desc=\"Processing test data\", unit=\"batch\"):\n        # Assuming 'input_ids' and 'attention_mask' are present in the batch\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        punct_feats = batch['punct_feats'].to(device)\n        \n        # Unwrap values from lists or tensors\n        filename = batch['filename'][0] if isinstance(batch['filename'], list) else batch['filename']\n        fileid = batch['id'].item() if isinstance(batch['id'], torch.Tensor) else batch['id']\n        src_text = batch['src'][0] if isinstance(batch['src'], list) else batch['src']\n        tgt_text = batch['tgt'][0] if isinstance(batch['tgt'], list) else batch['tgt']\n\n        \n        # Set the language token for the source\n        input_ids[0][0] = tokenizer.lang_code_to_id[SRC_LANG]\n        \n        # Run the encoder manually\n        with torch.no_grad():\n            encoder_outputs = model.model.encoder(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                return_dict=True\n            )\n    \n        try:\n            # Add punctuation features to encoder hidden states\n            combined_hidden_state = encoder_outputs.last_hidden_state\n        except:\n            skip2+=1\n            # print('skipping :(')\n            continue\n    \n        # Wrap into BaseModelOutput\n        encoder_outputs = BaseModelOutput(last_hidden_state=combined_hidden_state)\n    \n        # Generate translation from decoder with forced beginning token for Telugu\n        generated_tokens = model.generate(\n            input_ids=None,\n            encoder_outputs=encoder_outputs,\n            attention_mask=None,\n            forced_bos_token_id=tokenizer.lang_code_to_id[TGT_LANG],\n            max_length=50\n        )\n        \n        # Decode the generated tokens into text\n        translated_text = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n        translations.append(translated_text)\n        # print(translated_text)\n    \n        writer.writerow({\n                    \"filename\": filename,\n                    \"id\": fileid,\n                    \"src\": src_text,\n                    \"tgt\": tgt_text,\n                    \"translated\": translated_text\n                })\n    \n    \n    # Print out translations for the entire dev set\n    for translated_text in translations:\n        print(translated_text)\n        break\n    \n    print(skip2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T09:32:10.347435Z","iopub.execute_input":"2025-05-05T09:32:10.347738Z","iopub.status.idle":"2025-05-05T09:32:46.935245Z","shell.execute_reply.started":"2025-05-05T09:32:10.347718Z","shell.execute_reply":"2025-05-05T09:32:46.934588Z"}},"outputs":[{"name":"stderr","text":"Processing test data: 100%|██████████| 61/61 [00:36<00:00,  1.67batch/s]","output_type":"stream"},{"name":"stdout","text":"మురుగులు గాలిలోకి తీసుకొని మొదటి జంతువులు. ఫ్లైట్ చేయడానికి వారి సామర్థ్యం శత్రువులను మరింత సులభంగా బయలుకోడానికి మరియు ఆహారను ఇంకా సభ్యులను మరింత సమర్థవంతంగా క\n0\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\nimport torch\nimport pandas as pd\nimport csv\n\nmodel.eval()\n\n# Define the source and target languages\nSRC_LANG = \"en_XX\"\nTGT_LANG = \"te_IN\"\n\n# Set the tokenizer source language\ntokenizer.src_lang = SRC_LANG\n\n# Placeholder for storing translations\ntranslations = []\n\n# Prepare CSV file (write header once)\noutput_csv = \"both_test_asr_translations_final_returns.csv\"\nwith open(output_csv, mode='w', newline='', encoding='utf-8') as f:\n    writer = csv.DictWriter(f, fieldnames=[\"filename\", \"id\", \"src\", \"tgt\", \"translated\"])\n    writer.writeheader()\n\n    skip2=0\n    # Iterate over the dev_loader\n    for batch in tqdm(test_asr_dataloader, desc=\"Processing test data\", unit=\"batch\"):\n        # Assuming 'input_ids' and 'attention_mask' are present in the batch\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        punct_feats = batch['punct_feats'].to(device)\n        \n        # Unwrap values from lists or tensors\n        filename = batch['filename'][0] if isinstance(batch['filename'], list) else batch['filename']\n        fileid = batch['id'].item() if isinstance(batch['id'], torch.Tensor) else batch['id']\n        src_text = batch['src'][0] if isinstance(batch['src'], list) else batch['src']\n        tgt_text = batch['tgt'][0] if isinstance(batch['tgt'], list) else batch['tgt']\n\n        \n        # Set the language token for the source\n        input_ids[0][0] = tokenizer.lang_code_to_id[SRC_LANG]\n        \n        # Run the encoder manually\n        with torch.no_grad():\n            encoder_outputs = model.model.encoder(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                return_dict=True\n            )\n    \n        try:\n            # Add punctuation features to encoder hidden states\n            combined_hidden_state = encoder_outputs.last_hidden_state\n        except:\n            skip2+=1\n            # print('skipping :(')\n            continue\n    \n        # Wrap into BaseModelOutput\n        encoder_outputs = BaseModelOutput(last_hidden_state=combined_hidden_state)\n    \n        # Generate translation from decoder with forced beginning token for Telugu\n        generated_tokens = model.generate(\n            input_ids=None,\n            encoder_outputs=encoder_outputs,\n            attention_mask=None,\n            forced_bos_token_id=tokenizer.lang_code_to_id[TGT_LANG],\n            max_length=50\n        )\n        \n        # Decode the generated tokens into text\n        translated_text = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n        translations.append(translated_text)\n        # print(translated_text)\n    \n        writer.writerow({\n                    \"filename\": filename,\n                    \"id\": fileid,\n                    \"src\": src_text,\n                    \"tgt\": tgt_text,\n                    \"translated\": translated_text\n                })\n    \n    \n    # Print out translations for the entire dev set\n    for translated_text in translations:\n        print(translated_text)\n        break\n    \n    print(skip2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T09:32:52.929751Z","iopub.execute_input":"2025-05-05T09:32:52.930340Z","iopub.status.idle":"2025-05-05T09:33:48.225069Z","shell.execute_reply.started":"2025-05-05T09:32:52.930319Z","shell.execute_reply":"2025-05-05T09:33:48.224291Z"}},"outputs":[{"name":"stderr","text":"Processing test data: 100%|██████████| 85/85 [00:55<00:00,  1.54batch/s]","output_type":"stream"},{"name":"stdout","text":"పన్ను 2005 లో అబస్సెనిటీ యాటివిటీ కు కాంగ్రెస్ నిధులను అందించింది మరియు దీనిని తగిన ఔత్సాహిక పరుపులకు FBI 10 మంది ఏజెంట్లు నిరూపించాలని అన్నారు.\n0\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
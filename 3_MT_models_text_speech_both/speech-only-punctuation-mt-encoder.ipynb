{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import gdown\n\ncsv_file_id = '1-DfbuRqP5xe9eNMjobEDH_cZR1FlwyEw'\ngdown.download(f'https://drive.google.com/uc?export=download&id={csv_file_id}', 'MT_train.csv', quiet=False)\ntsv_file_id = '1pBYaqKbkJt66bJlBJGKvTaWVI6Q0yNCC'\ngdown.download(f'https://drive.google.com/uc?export=download&id={tsv_file_id}', 'train.tsv', quiet=False)\nmat_file_id = '1UQTiPe34L4VAOZj3OFOIl2FnHsf7y0nf'\ngdown.download(f'https://drive.google.com/uc?export=download&id={mat_file_id}', 'punctuation_train.mat', quiet=False)\n\n\ncsv_file_id = '1-I-VFAdiAvFF-7x_Y_0_TwDf0v2kl80l'\ngdown.download(f'https://drive.google.com/uc?export=download&id={csv_file_id}', 'MT_dev.csv', quiet=False)\ntsv_file_id = '1M6uGyGSOEW7wKirzi7dlizlRaN9wStwD'\ngdown.download(f'https://drive.google.com/uc?export=download&id={tsv_file_id}', 'dev.tsv', quiet=False)\nmat_file_id = '1veQfrTT9aTfxRwJ-Sf70QB-ae-kvca8Y'\ngdown.download(f'https://drive.google.com/uc?export=download&id={mat_file_id}', 'punctuation_dev.mat', quiet=False)\n\n\ncsv_file_id = '1-GVAKGJs92uzo8bF1d-q9J_apyqP1ofG'\ngdown.download(f'https://drive.google.com/uc?export=download&id={csv_file_id}', 'MT_test.csv', quiet=False)\ntsv_file_id = '1m7JIcj_-SVYASbcbBiT4LoPdgZdqGN4i'\ngdown.download(f'https://drive.google.com/uc?export=download&id={tsv_file_id}', 'test.tsv', quiet=False)\nmat_file_id = '1Z4Ya9JUCEtYXCiYs8LI9d21C4kSvb0KS'\ngdown.download(f'https://drive.google.com/uc?export=download&id={mat_file_id}', 'punctuation_test.mat', quiet=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T18:09:31.629877Z","iopub.execute_input":"2025-05-04T18:09:31.630586Z","iopub.status.idle":"2025-05-04T18:10:15.550999Z","shell.execute_reply.started":"2025-05-04T18:09:31.630550Z","shell.execute_reply":"2025-05-04T18:10:15.550249Z"}},"outputs":[{"name":"stderr","text":"Downloading...\nFrom: https://drive.google.com/uc?export=download&id=1-DfbuRqP5xe9eNMjobEDH_cZR1FlwyEw\nTo: /kaggle/working/MT_train.csv\n100%|██████████| 599k/599k [00:00<00:00, 104MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?export=download&id=1pBYaqKbkJt66bJlBJGKvTaWVI6Q0yNCC\nTo: /kaggle/working/train.tsv\n100%|██████████| 1.41M/1.41M [00:00<00:00, 140MB/s]\nDownloading...\nFrom (original): https://drive.google.com/uc?export=download&id=1UQTiPe34L4VAOZj3OFOIl2FnHsf7y0nf\nFrom (redirected): https://drive.google.com/uc?export=download&id=1UQTiPe34L4VAOZj3OFOIl2FnHsf7y0nf&confirm=t&uuid=60f6b9a7-d8ff-4624-be0b-ffcce2bed0cb\nTo: /kaggle/working/punctuation_train.mat\n100%|██████████| 173M/173M [00:02<00:00, 61.9MB/s] \nDownloading...\nFrom: https://drive.google.com/uc?export=download&id=1-I-VFAdiAvFF-7x_Y_0_TwDf0v2kl80l\nTo: /kaggle/working/MT_dev.csv\n100%|██████████| 66.2k/66.2k [00:00<00:00, 45.6MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?export=download&id=1M6uGyGSOEW7wKirzi7dlizlRaN9wStwD\nTo: /kaggle/working/dev.tsv\n100%|██████████| 213k/213k [00:00<00:00, 95.2MB/s]\nDownloading...\nFrom (original): https://drive.google.com/uc?export=download&id=1veQfrTT9aTfxRwJ-Sf70QB-ae-kvca8Y\nFrom (redirected): https://drive.google.com/uc?export=download&id=1veQfrTT9aTfxRwJ-Sf70QB-ae-kvca8Y&confirm=t&uuid=c5a6706e-18a4-46a8-b881-244204cf4d1f\nTo: /kaggle/working/punctuation_dev.mat\n100%|██████████| 28.5M/28.5M [00:00<00:00, 41.5MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?export=download&id=1-GVAKGJs92uzo8bF1d-q9J_apyqP1ofG\nTo: /kaggle/working/MT_test.csv\n100%|██████████| 148k/148k [00:00<00:00, 91.1MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?export=download&id=1m7JIcj_-SVYASbcbBiT4LoPdgZdqGN4i\nTo: /kaggle/working/test.tsv\n100%|██████████| 368k/368k [00:00<00:00, 110MB/s]\nDownloading...\nFrom (original): https://drive.google.com/uc?export=download&id=1Z4Ya9JUCEtYXCiYs8LI9d21C4kSvb0KS\nFrom (redirected): https://drive.google.com/uc?export=download&id=1Z4Ya9JUCEtYXCiYs8LI9d21C4kSvb0KS&confirm=t&uuid=19e21980-b4d3-47a0-ad67-25f364d77234\nTo: /kaggle/working/punctuation_test.mat\n100%|██████████| 46.8M/46.8M [00:00<00:00, 47.1MB/s]\n","output_type":"stream"},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"'punctuation_test.mat'"},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"import os\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom scipy.io import loadmat\nfrom transformers import MBartTokenizer\n\nclass EnglishTeluguPunctDataset:\n    def __init__(self, mat_path, tsv_path, csv_path, tokenizer, max_length=128):\n        self.mat_path = mat_path\n        self.tsv_path = tsv_path\n        self.csv_path = csv_path\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.skipped = 0\n        self.skipped_files_names=[]\n\n        # Load .mat file\n        mat_data = loadmat(mat_path)\n        self.features = mat_data['features'].flatten()\n        self.filenames = mat_data['filenames'].flatten()\n        \n        # Convert filenames to list of strings\n        self.filename_to_feat = {}\n        for i, fname in enumerate(self.filenames):\n            fname_str = str(fname[0]) if isinstance(fname, (np.ndarray, list)) else str(fname)\n            self.filename_to_feat[fname_str] = self.features[i]\n\n        # Load MT_data.csv (English + Telugu)\n        mt_data = pd.read_csv(csv_path)\n        self.id_to_english = mt_data.set_index(\"id\")[\"english\"].to_dict()\n        self.id_to_telugu = mt_data.set_index(\"id\")[\"telugu\"].to_dict()\n\n        # # Load TSV file (filename → ID mapping)\n        # eng_tsv = pd.read_csv(tsv_path, sep=\"\\t\", header=None, dtype=str)\n        # eng_tsv = eng_tsv[eng_tsv.columns[:2]]\n        # eng_tsv.columns = [\"id\", \"filename\"]\n        # eng_tsv[\"id\"] = eng_tsv[\"id\"].astype(int)\n        # self.filename_to_id = eng_tsv.set_index(\"filename\")[\"id\"].to_dict()\n\n        # Load TSV and filter to only include filenames present in .mat file\n        eng_tsv = pd.read_csv(tsv_path, sep=\"\\t\", header=None, dtype=str)\n        eng_tsv = eng_tsv[eng_tsv.columns[:2]]\n        eng_tsv.columns = [\"id\", \"filename\"]\n        eng_tsv[\"id\"] = eng_tsv[\"id\"].astype(int)\n        \n        # Only keep rows where filename exists in .mat file\n        valid_filenames_set = set(self.filename_to_feat.keys())\n        eng_tsv = eng_tsv[eng_tsv[\"filename\"].isin(valid_filenames_set)]\n        \n        # Build mapping\n        self.filename_to_id = eng_tsv.set_index(\"filename\")[\"id\"].to_dict()\n\n\n        # Build valid entries list\n        self.valid_entries = []\n        for filename, id_ in self.filename_to_id.items():\n            if id_ in self.id_to_english and id_ in self.id_to_telugu and filename in self.filename_to_feat:\n                self.valid_entries.append((filename, id_))\n            else:\n                self.skipped_files_names.append(filename)\n                self.skipped += 1\n\n    def __len__(self):\n        return len(self.valid_entries)\n\n    def __getitem__(self, idx):\n        filename, id_ = self.valid_entries[idx]\n\n        # Load punctuation features\n        punct_feats_np = self.filename_to_feat[filename]\n        punct_feats = torch.tensor(punct_feats_np, dtype=torch.float32)\n        zero_row = torch.zeros((1, punct_feats.size(1)), dtype=torch.float32)\n        punct_feats = torch.cat([zero_row, punct_feats, zero_row], dim=0)  \n\n        # Get texts\n        src_text = self.id_to_english[id_]\n        tgt_text = self.id_to_telugu[id_]\n\n        # Tokenize\n        src_enc = self.tokenizer(src_text, return_tensors=\"pt\", padding=False, truncation=True)\n        tgt_enc = self.tokenizer(tgt_text, return_tensors=\"pt\", padding=False, truncation=True)\n\n        # Add language code tokens\n        src_enc.input_ids[0][0] = self.tokenizer.lang_code_to_id[\"en_XX\"]\n        tgt_enc.input_ids[0][0] = self.tokenizer.lang_code_to_id[\"te_IN\"]\n\n        return {\n            \"input_ids\": src_enc.input_ids.squeeze(0),\n            \"attention_mask\": src_enc.attention_mask.squeeze(0),\n            \"labels\": tgt_enc.input_ids.squeeze(0),\n            \"punct_feats\": punct_feats,\n            \"src\": src_text,\n            \"tgt\": tgt_text,\n            \"filename\": filename,\n            \"id\": id_\n        }\n\n    def get_skipped_count(self):\n        # filename, id_ = self.valid_entries[idx]\n        # self.skipped_files_names.append(filename)\n        return self.skipped, self.skipped_files_names\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T18:49:44.027218Z","iopub.execute_input":"2025-05-04T18:49:44.027673Z","iopub.status.idle":"2025-05-04T18:49:44.040053Z","shell.execute_reply.started":"2025-05-04T18:49:44.027653Z","shell.execute_reply":"2025-05-04T18:49:44.039311Z"}},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":"# model init","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom transformers import MBartForConditionalGeneration, MBart50TokenizerFast\nfrom torch.optim import AdamW\nfrom tqdm import tqdm\nfrom transformers.modeling_outputs import BaseModelOutput\nfrom torch.utils.data import DataLoader\n\n\n\nmodel_name = \"facebook/mbart-large-50-many-to-many-mmt\"\n# Load tokenizer and model\n# tokenizer = MBart50TokenizerFast.from_pretrained(model_name)\nmodel = MBartForConditionalGeneration.from_pretrained(model_name)\n\n# Initialize the tokenizer\ntokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T18:10:44.612628Z","iopub.execute_input":"2025-05-04T18:10:44.613263Z","iopub.status.idle":"2025-05-04T18:11:17.773650Z","shell.execute_reply.started":"2025-05-04T18:10:44.613232Z","shell.execute_reply":"2025-05-04T18:11:17.773012Z"}},"outputs":[{"name":"stderr","text":"2025-05-04 18:10:51.387106: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1746382251.561912      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1746382251.611514      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.43k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a83dba57ab9a4d1ba498e4cc17da7d4d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.44G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5832e9bd50cd465c99362498db72e0d3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/261 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16c38cf365c54e2a824faffff4ad13bd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/529 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"93f420ceb8784917bd7c0279014947e8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b51fd25be6a40c0a66a4aacf6e6af29"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/649 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a180cb4499947b78f21187eb3850fe1"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"# Initialize the dataset\ntrain_dataset = EnglishTeluguPunctDataset(mat_path=\"punctuation_train.mat\",\n                                    tsv_path=\"train.tsv\",\n                                    csv_path=\"MT_train.csv\",\n                                    tokenizer=tokenizer)\n# Create DataLoader\ntrain_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n\n# Initialize the dataset\ndev_dataset = EnglishTeluguPunctDataset(mat_path=\"punctuation_dev.mat\",\n                                    tsv_path=\"dev.tsv\",\n                                    csv_path=\"MT_dev.csv\",\n                                    tokenizer=tokenizer)\n\n# Create DataLoader\ndev_dataloader = DataLoader(dev_dataset, batch_size=1, shuffle=True)\n\n\n\n# Initialize the dataset\ntest_dataset = EnglishTeluguPunctDataset(mat_path=\"punctuation_test.mat\",\n                                    tsv_path=\"test.tsv\",\n                                    csv_path=\"MT_test.csv\",\n                                    tokenizer=tokenizer)\n\n# Create DataLoader\ntest_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T18:49:48.458708Z","iopub.execute_input":"2025-05-04T18:49:48.459083Z","iopub.status.idle":"2025-05-04T18:49:48.757746Z","shell.execute_reply.started":"2025-05-04T18:49:48.459052Z","shell.execute_reply":"2025-05-04T18:49:48.757076Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"print(len(train_dataset))\nprint(len(dev_dataset))\nprint(len(test_dataset))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T18:11:32.773252Z","iopub.execute_input":"2025-05-04T18:11:32.773566Z","iopub.status.idle":"2025-05-04T18:11:32.777957Z","shell.execute_reply.started":"2025-05-04T18:11:32.773543Z","shell.execute_reply":"2025-05-04T18:11:32.777135Z"}},"outputs":[{"name":"stdout","text":"699\n113\n164\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"SRC_LANG = \"en_XX\"\nTGT_LANG = \"te_IN\"\n\ntokenizer.src_lang = SRC_LANG\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Set model to training mode\nmodel.train()\n\noptimizer = AdamW(model.parameters(), lr=1e-5)\n\nimport torch.nn as nn\n\nfrom transformers.modeling_outputs import BaseModelOutput\nskip=0\n\nimport torch.nn as nn\n\n# Assume punct_feats shape: (batch_size, seq_len, feat_dim)\nfeat_dim = train_dataset[0][\"punct_feats\"].shape[1]\nhidden_dim = model.model.encoder.config.d_model  # mBART hidden size (e.g., 1024)\n\n# Punctuation feature encoder\npunct_feat_encoder = nn.Linear(feat_dim, hidden_dim).to(device)\n\n# Add to optimizer\noptimizer = AdamW(list(model.parameters()) + list(punct_feat_encoder.parameters()), lr=5e-5)\n\n\nfor epoch in range(3):\n    model.train()\n    punct_feat_encoder.train()\n\n    for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}\"):\n        optimizer.zero_grad()\n\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n        punct_feats = batch['punct_feats'].to(device)  # (batch, seq_len, feat_dim)\n\n        # Forward encoder\n        encoder_outputs = model.model.encoder(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            return_dict=True\n        )\n\n        # Pass punctuation features through its encoder\n        try:\n            encoded_punct_feats = punct_feat_encoder(punct_feats)  # (batch, seq_len, hidden_dim)\n            combined_hidden_state = encoder_outputs.last_hidden_state + encoded_punct_feats *0.1\n        except Exception as e:\n            skip += 1\n            continue\n\n        encoder_outputs = BaseModelOutput(last_hidden_state=combined_hidden_state)\n        decoder_input_ids = model.prepare_decoder_input_ids_from_labels(labels)\n\n        outputs = model(\n            input_ids=None,\n            attention_mask=None,\n            decoder_input_ids=decoder_input_ids,\n            encoder_outputs=encoder_outputs,\n            labels=labels,\n            return_dict=True\n        )\n\n        loss = outputs.loss\n        if torch.isnan(loss):\n            continue\n        loss.backward()\n        optimizer.step()\n\n    print(f\"Epoch {epoch + 1}, Loss: {loss.item():.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T18:12:00.385946Z","iopub.execute_input":"2025-05-04T18:12:00.386227Z","iopub.status.idle":"2025-05-04T18:23:06.333376Z","shell.execute_reply.started":"2025-05-04T18:12:00.386206Z","shell.execute_reply":"2025-05-04T18:23:06.332589Z"}},"outputs":[{"name":"stderr","text":"Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\nEpoch 1: 100%|██████████| 699/699 [03:41<00:00,  3.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1, Loss: 4.8411\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2: 100%|██████████| 699/699 [03:41<00:00,  3.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2, Loss: 4.2137\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3: 100%|██████████| 699/699 [03:41<00:00,  3.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3, Loss: 0.7976\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"torch.save(model.state_dict(), \"mbart_en_te_speech_punct_with_encoder.pt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T18:23:54.128492Z","iopub.execute_input":"2025-05-04T18:23:54.129204Z","iopub.status.idle":"2025-05-04T18:23:58.034228Z","shell.execute_reply.started":"2025-05-04T18:23:54.129181Z","shell.execute_reply":"2025-05-04T18:23:58.033625Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"from IPython.display import FileLink\n\n# Replace with your actual .pt file name\nFileLink('mbart_en_te_speech_punct_with_encoder.pt')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T18:23:58.035705Z","iopub.execute_input":"2025-05-04T18:23:58.035910Z","iopub.status.idle":"2025-05-04T18:23:58.040901Z","shell.execute_reply.started":"2025-05-04T18:23:58.035894Z","shell.execute_reply":"2025-05-04T18:23:58.040391Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/mbart_en_te_speech_punct_with_encoder.pt","text/html":"<a href='mbart_en_te_speech_punct_with_encoder.pt' target='_blank'>mbart_en_te_speech_punct_with_encoder.pt</a><br>"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\nimport torch\nimport pandas as pd\nimport csv\n\n# Assuming model and tokenizer are already loaded\n# model = MBartForConditionalGeneration.from_pretrained(model_path)\n# tokenizer = MBart50TokenizerFast.from_pretrained(model_path)\n\nmodel.eval()\n\n# Define the source and target languages\nSRC_LANG = \"en_XX\"\nTGT_LANG = \"te_IN\"\n\n# Set the tokenizer source language\ntokenizer.src_lang = SRC_LANG\n\n# Placeholder for storing translations\ntranslations = []\n\n# Prepare CSV file (write header once)\noutput_csv = \"dev_translations_speech_encoder.csv\"\nwith open(output_csv, mode='w', newline='', encoding='utf-8') as f:\n    writer = csv.DictWriter(f, fieldnames=[\"filename\", \"id\", \"src\", \"tgt\", \"translated\"])\n    writer.writeheader()\n\n    skip2=0\n    # Iterate over the dev_loader\n    for batch in tqdm(dev_dataloader, desc=\"Processing test data\", unit=\"batch\"):\n        # Assuming 'input_ids' and 'attention_mask' are present in the batch\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        punct_feats = batch['punct_feats'].to(device)\n        \n        # Unwrap values from lists or tensors\n        filename = batch['filename'][0] if isinstance(batch['filename'], list) else batch['filename']\n        fileid = batch['id'].item() if isinstance(batch['id'], torch.Tensor) else batch['id']\n        src_text = batch['src'][0] if isinstance(batch['src'], list) else batch['src']\n        tgt_text = batch['tgt'][0] if isinstance(batch['tgt'], list) else batch['tgt']\n\n        \n        # Set the language token for the source\n        input_ids[0][0] = tokenizer.lang_code_to_id[SRC_LANG]\n        \n        # Run the encoder manually\n        with torch.no_grad():\n            encoder_outputs = model.model.encoder(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                return_dict=True\n            )\n    \n        try:\n            # Add punctuation features to encoder hidden states\n            combined_hidden_state = encoder_outputs.last_hidden_state\n        except:\n            skip2+=1\n            # print('skipping :(')\n            continue\n    \n        # Wrap into BaseModelOutput\n        encoder_outputs = BaseModelOutput(last_hidden_state=combined_hidden_state)\n    \n        # Generate translation from decoder with forced beginning token for Telugu\n        generated_tokens = model.generate(\n            input_ids=None,\n            encoder_outputs=encoder_outputs,\n            attention_mask=None,\n            forced_bos_token_id=tokenizer.lang_code_to_id[TGT_LANG],\n            max_length=50\n        )\n        \n        # Decode the generated tokens into text\n        translated_text = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n        # print(translated_text)\n        translations.append(translated_text)\n    \n        writer.writerow({\n                    \"filename\": filename,\n                    \"id\": fileid,\n                    \"src\": src_text,\n                    \"tgt\": tgt_text,\n                    \"translated\": translated_text\n                })\n    \n    \n    # Print out translations for the entire dev set\n    for translated_text in translations:\n        print(translated_text)\n        break\n    \n    print(skip2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T18:24:32.450072Z","iopub.execute_input":"2025-05-04T18:24:32.450624Z","iopub.status.idle":"2025-05-04T18:25:51.624877Z","shell.execute_reply.started":"2025-05-04T18:24:32.450602Z","shell.execute_reply":"2025-05-04T18:25:51.624093Z"}},"outputs":[{"name":"stderr","text":"Processing test data: 100%|██████████| 113/113 [01:19<00:00,  1.43batch/s]","output_type":"stream"},{"name":"stdout","text":"వేల సంవత్సరాల క్రితం, ఆర్రి స్టార్క్యూస్ అనే మనిషి సోలార్ సిస్టమ్ సూర్యుని చుట్టూ మారినారని చెప్పాడు.\n0\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"import pandas as pd\ndf= pd.read_csv(\"dev_translations_speech_encoder.csv\")\nprint(df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T19:12:42.423896Z","iopub.execute_input":"2025-05-04T19:12:42.424160Z","iopub.status.idle":"2025-05-04T19:12:42.434382Z","shell.execute_reply.started":"2025-05-04T19:12:42.424140Z","shell.execute_reply":"2025-05-04T19:12:42.433677Z"}},"outputs":[{"name":"stdout","text":"                   filename    id  \\\n0  10749355711908235873.wav  1615   \n1  10354232920158081925.wav  1560   \n2  16799907084202296752.wav  1658   \n3  15553404816205838782.wav  1592   \n4  14780052696556980882.wav  1624   \n\n                                                 src  \\\n0  Thousands of years ago, a man called Aristarch...   \n1  As soon as you get out of the current, swimmin...   \n2  With only eighteen medals available a day, a n...   \n3  When you went abroad at first, people were pro...   \n4  Inland waterways can be a good theme to base a...   \n\n                                                 tgt  \\\n0  వేల సంవత్సరాల క్రితం అరిస్టార్కస్ అనే వ్యక్తి ...   \n1  మీరు కరెంట్ నుంచి బయటకు వచ్చిన వెంటనే, తిరిగి ...   \n2  రోజుకు కేవలం పద్దెనిమిది మెడల్స్ మాత్రమే అందుబ...   \n3  మీరు మొదట విదేశాలకు వెళ్ళినప్పుడు, కొత్త దేశంల...   \n4  సెలవుదినం గడపడానికి ఇన్ల్యాండ్ వాటర్​వేస్ మంచి...   \n\n                                          translated  \n0  వేల సంవత్సరాల క్రితం, ఆర్రి స్టార్క్యూస్ అనే మ...  \n1  మీరు ప్రవాహానికి నుంచి బయటకు వెళ్ళికొద్దీ, సాధ...  \n2  ఏద్వారం మాత్రమే పద్వీపాలు ఉన్నందుతో, అనేక దేశా...  \n3  మీరుమొదటికి విదేశానికి వెళ్ళినప్పుడు, ప్రజలు బ...  \n4  లోతట్టు జలమార్గలు చుట్టూ వేసవి కోసం ఒక మంచిథ్య...  \n","output_type":"stream"}],"execution_count":79},{"cell_type":"code","source":"from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\nimport torch\nimport pandas as pd\nimport csv\n\n# Assuming model and tokenizer are already loaded\n# model = MBartForConditionalGeneration.from_pretrained(model_path)\n# tokenizer = MBart50TokenizerFast.from_pretrained(model_path)\n\nmodel.eval()\n\n# Define the source and target languages\nSRC_LANG = \"en_XX\"\nTGT_LANG = \"te_IN\"\n\n# Set the tokenizer source language\ntokenizer.src_lang = SRC_LANG\n\n# Placeholder for storing translations\ntranslations = []\n\n# Prepare CSV file (write header once)\noutput_csv = \"test_translations_speech_encoder.csv\"\nwith open(output_csv, mode='w', newline='', encoding='utf-8') as f:\n    writer = csv.DictWriter(f, fieldnames=[\"filename\", \"id\", \"src\", \"tgt\", \"translated\"])\n    writer.writeheader()\n\n    skip2=0\n    # Iterate over the dev_loader\n    for batch in tqdm(test_dataloader, desc=\"Processing test data\", unit=\"batch\"):\n        # Assuming 'input_ids' and 'attention_mask' are present in the batch\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        punct_feats = batch['punct_feats'].to(device)\n        \n        # Unwrap values from lists or tensors\n        filename = batch['filename'][0] if isinstance(batch['filename'], list) else batch['filename']\n        fileid = batch['id'].item() if isinstance(batch['id'], torch.Tensor) else batch['id']\n        src_text = batch['src'][0] if isinstance(batch['src'], list) else batch['src']\n        tgt_text = batch['tgt'][0] if isinstance(batch['tgt'], list) else batch['tgt']\n\n        \n        # Set the language token for the source\n        input_ids[0][0] = tokenizer.lang_code_to_id[SRC_LANG]\n        \n        # Run the encoder manually\n        with torch.no_grad():\n            encoder_outputs = model.model.encoder(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                return_dict=True\n            )\n    \n        try:\n            # Add punctuation features to encoder hidden states\n            combined_hidden_state = encoder_outputs.last_hidden_state\n        except:\n            skip2+=1\n            # print('skipping :(')\n            continue\n    \n        # Wrap into BaseModelOutput\n        encoder_outputs = BaseModelOutput(last_hidden_state=combined_hidden_state)\n    \n        # Generate translation from decoder with forced beginning token for Telugu\n        generated_tokens = model.generate(\n            input_ids=None,\n            encoder_outputs=encoder_outputs,\n            attention_mask=None,\n            forced_bos_token_id=tokenizer.lang_code_to_id[TGT_LANG],\n            max_length=50\n        )\n        \n        # Decode the generated tokens into text\n        translated_text = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n        translations.append(translated_text)\n    \n        writer.writerow({\n                    \"filename\": filename,\n                    \"id\": fileid,\n                    \"src\": src_text,\n                    \"tgt\": tgt_text,\n                    \"translated\": translated_text\n                })\n    \n    \n    # Print out translations for the entire dev set\n    for translated_text in translations:\n        print(translated_text)\n        break\n    \n    print(skip2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T19:12:32.724953Z","iopub.execute_input":"2025-05-04T19:12:32.725549Z","iopub.status.idle":"2025-05-04T19:12:38.676387Z","shell.execute_reply.started":"2025-05-04T19:12:32.725521Z","shell.execute_reply":"2025-05-04T19:12:38.675326Z"}},"outputs":[{"name":"stderr","text":"Processing test data:   5%|▍         | 8/164 [00:05<01:55,  1.35batch/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/763363343.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;31m# Generate translation from decoder with forced beginning token for Telugu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         generated_tokens = model.generate(\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001b[0m\n\u001b[1;32m   2480\u001b[0m             )\n\u001b[1;32m   2481\u001b[0m             \u001b[0;31m# 12. run beam sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2482\u001b[0;31m             result = self._beam_search(\n\u001b[0m\u001b[1;32m   2483\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2484\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepared_logits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_beam_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   3891\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3892\u001b[0m         \u001b[0;31m# 4. run the generation loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3893\u001b[0;31m         \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_unfinished_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis_peer_finished\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msynced_gpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3894\u001b[0m             \u001b[0;31m# a. Forward current tokens, obtain the logits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3895\u001b[0m             \u001b[0mflat_running_sequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flatten_beam_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrunning_sequences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mcur_len\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_has_unfinished_sequences\u001b[0;34m(self, this_peer_finished, synced_gpus, device)\u001b[0m\n\u001b[1;32m   2598\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2599\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2600\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m_has_unfinished_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthis_peer_finished\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msynced_gpus\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2601\u001b[0m         \"\"\"\n\u001b[1;32m   2602\u001b[0m         \u001b[0mReturns\u001b[0m \u001b[0mwhether\u001b[0m \u001b[0mthere\u001b[0m \u001b[0mare\u001b[0m \u001b[0mstill\u001b[0m \u001b[0munfinished\u001b[0m \u001b[0msequences\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mexistence\u001b[0m \u001b[0mof\u001b[0m \u001b[0munfinished\u001b[0m \u001b[0msequences\u001b[0m \u001b[0;32mis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":78},{"cell_type":"code","source":"batch = next(iter(train_dataloader))\nprint(batch['src'])  # This will give you the 'src' field from the first batch\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T19:13:11.904440Z","iopub.execute_input":"2025-05-04T19:13:11.904933Z","iopub.status.idle":"2025-05-04T19:13:11.911333Z","shell.execute_reply.started":"2025-05-04T19:13:11.904911Z","shell.execute_reply":"2025-05-04T19:13:11.910659Z"}},"outputs":[{"name":"stdout","text":"['The village of Haldarsvík offer views of the nearby island Eysturoy and has an unusual octagonal church.']\n","output_type":"stream"}],"execution_count":81},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"csv_file_id = '1FR-MyJRxZH62grW7DihFqbgA8HG0Wnne'\ngdown.download(f'https://drive.google.com/uc?export=download&id={csv_file_id}', 'asr_decoded_dev.csv', quiet=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T19:13:14.131462Z","iopub.execute_input":"2025-05-04T19:13:14.131756Z","iopub.status.idle":"2025-05-04T19:13:17.843818Z","shell.execute_reply.started":"2025-05-04T19:13:14.131735Z","shell.execute_reply":"2025-05-04T19:13:17.843214Z"}},"outputs":[{"name":"stderr","text":"Downloading...\nFrom: https://drive.google.com/uc?export=download&id=1FR-MyJRxZH62grW7DihFqbgA8HG0Wnne\nTo: /kaggle/working/asr_decoded_dev.csv\n100%|██████████| 227k/227k [00:00<00:00, 82.8MB/s]\n","output_type":"stream"},{"execution_count":82,"output_type":"execute_result","data":{"text/plain":"'asr_decoded_dev.csv'"},"metadata":{}}],"execution_count":82},{"cell_type":"code","source":"import pandas as pd\nimport torch\n\ndef update_dataset_with_asr(dataset, asr_csv_path, tokenizer, src_lang=\"en_XX\"):\n    # Load the ASR CSV\n    asr_df = pd.read_csv(asr_csv_path)\n    filename_to_asr = dict(zip(asr_df[\"filename\"], asr_df[\"asr_decoded\"]))\n    \n    # Loop through the dataset and modify the fields\n    updated_entries = []\n    \n    for entry in dataset:\n        filename = entry[\"filename\"]  # Get filename from the dataset\n        \n        if filename in filename_to_asr:\n            # Get the corresponding ASR decoded text from CSV\n            asr_text = filename_to_asr[filename]\n            \n            # Tokenize the new ASR text\n            enc = tokenizer(asr_text, return_tensors=\"pt\", padding=False, truncation=True)\n            enc.input_ids[0][0] = tokenizer.lang_code_to_id[src_lang]  # Add language code to first token\n            \n            # Update the dataset entry\n            entry[\"input_ids\"] = enc.input_ids.squeeze(0)\n            entry[\"attention_mask\"] = enc.attention_mask.squeeze(0)\n            entry[\"src\"] = asr_text\n            \n            updated_entries.append(entry)\n        else:\n            # If no match is found, keep the original entry\n            updated_entries.append(entry)\n    \n    return updated_entries\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T19:13:17.844743Z","iopub.execute_input":"2025-05-04T19:13:17.844972Z","iopub.status.idle":"2025-05-04T19:13:17.850774Z","shell.execute_reply.started":"2025-05-04T19:13:17.844956Z","shell.execute_reply":"2025-05-04T19:13:17.850114Z"}},"outputs":[],"execution_count":83},{"cell_type":"code","source":"dev_asr_dataset = update_dataset_with_asr(dev_dataset, \"asr_decoded_dev.csv\", tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T19:08:43.094457Z","iopub.execute_input":"2025-05-04T19:08:43.094887Z","iopub.status.idle":"2025-05-04T19:08:43.234236Z","shell.execute_reply.started":"2025-05-04T19:08:43.094868Z","shell.execute_reply":"2025-05-04T19:08:43.233651Z"}},"outputs":[],"execution_count":66},{"cell_type":"code","source":"print(len(dev_asr_dataset))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T19:08:43.508081Z","iopub.execute_input":"2025-05-04T19:08:43.508619Z","iopub.status.idle":"2025-05-04T19:08:43.512502Z","shell.execute_reply.started":"2025-05-04T19:08:43.508599Z","shell.execute_reply":"2025-05-04T19:08:43.511820Z"}},"outputs":[{"name":"stdout","text":"113\n","output_type":"stream"}],"execution_count":67},{"cell_type":"code","source":"csv_file_id = '1e0JBRmo98zKK8Lbfof9QFZ7IlQiYtDC2'\ngdown.download(f'https://drive.google.com/uc?export=download&id={csv_file_id}', 'asr_decoded_test.csv', quiet=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T19:04:21.834924Z","iopub.execute_input":"2025-05-04T19:04:21.835512Z","iopub.status.idle":"2025-05-04T19:04:25.522203Z","shell.execute_reply.started":"2025-05-04T19:04:21.835490Z","shell.execute_reply":"2025-05-04T19:04:25.521628Z"}},"outputs":[{"name":"stderr","text":"Downloading...\nFrom: https://drive.google.com/uc?export=download&id=1e0JBRmo98zKK8Lbfof9QFZ7IlQiYtDC2\nTo: /kaggle/working/asr_decoded_test.csv\n100%|██████████| 356k/356k [00:00<00:00, 92.9MB/s]\n","output_type":"stream"},{"execution_count":58,"output_type":"execute_result","data":{"text/plain":"'asr_decoded_test.csv'"},"metadata":{}}],"execution_count":58},{"cell_type":"code","source":"test_asr_dataset = update_dataset_with_asr(test_dataset, \"asr_decoded_test.csv\", tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T19:08:49.361237Z","iopub.execute_input":"2025-05-04T19:08:49.361877Z","iopub.status.idle":"2025-05-04T19:08:49.582054Z","shell.execute_reply.started":"2025-05-04T19:08:49.361850Z","shell.execute_reply":"2025-05-04T19:08:49.581462Z"}},"outputs":[],"execution_count":68},{"cell_type":"code","source":"print(len(test_asr_dataset))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T19:08:50.921743Z","iopub.execute_input":"2025-05-04T19:08:50.922388Z","iopub.status.idle":"2025-05-04T19:08:50.925854Z","shell.execute_reply.started":"2025-05-04T19:08:50.922368Z","shell.execute_reply":"2025-05-04T19:08:50.925241Z"}},"outputs":[{"name":"stdout","text":"164\n","output_type":"stream"}],"execution_count":69},{"cell_type":"code","source":"dev_asr_dataloader = DataLoader(dev_asr_dataset, batch_size=1, shuffle=True)\ntest_asr_dataloader = DataLoader(test_asr_dataset, batch_size=1, shuffle=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T19:08:52.026794Z","iopub.execute_input":"2025-05-04T19:08:52.027420Z","iopub.status.idle":"2025-05-04T19:08:52.033183Z","shell.execute_reply.started":"2025-05-04T19:08:52.027399Z","shell.execute_reply":"2025-05-04T19:08:52.032608Z"}},"outputs":[],"execution_count":70},{"cell_type":"code","source":"\nmodel.eval()\n\n# Define the source and target languages\nSRC_LANG = \"en_XX\"\nTGT_LANG = \"te_IN\"\n\n# Set the tokenizer source language\ntokenizer.src_lang = SRC_LANG\n\n# Placeholder for storing translations\ntranslations = []\n\n# Prepare CSV file (write header once)\noutput_csv = \"dev_asr_translations_speech_encoder.csv\"\nwith open(output_csv, mode='w', newline='', encoding='utf-8') as f:\n    writer = csv.DictWriter(f, fieldnames=[\"filename\", \"id\", \"src\", \"tgt\", \"translated\"])\n    writer.writeheader()\n\n    skip2=0\n    # Iterate over the dev_loader\n    for batch in tqdm(dev_asr_dataloader, desc=\"Processing test data\", unit=\"batch\"):\n        # Assuming 'input_ids' and 'attention_mask' are present in the batch\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        punct_feats = batch['punct_feats'].to(device)\n        \n        # Unwrap values from lists or tensors\n        filename = batch['filename'][0] if isinstance(batch['filename'], list) else batch['filename']\n        fileid = batch['id'].item() if isinstance(batch['id'], torch.Tensor) else batch['id']\n        src_text = batch['src'][0] if isinstance(batch['src'], list) else batch['src']\n        tgt_text = batch['tgt'][0] if isinstance(batch['tgt'], list) else batch['tgt']\n\n        \n        # Set the language token for the source\n        input_ids[0][0] = tokenizer.lang_code_to_id[SRC_LANG]\n        \n        # Run the encoder manually\n        with torch.no_grad():\n            encoder_outputs = model.model.encoder(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                return_dict=True\n            )\n    \n        try:\n            # Add punctuation features to encoder hidden states\n            combined_hidden_state = encoder_outputs.last_hidden_state\n        except:\n            skip2+=1\n            # print('skipping :(')\n            continue\n    \n        # Wrap into BaseModelOutput\n        encoder_outputs = BaseModelOutput(last_hidden_state=combined_hidden_state)\n    \n        # Generate translation from decoder with forced beginning token for Telugu\n        generated_tokens = model.generate(\n            input_ids=None,\n            encoder_outputs=encoder_outputs,\n            attention_mask=None,\n            forced_bos_token_id=tokenizer.lang_code_to_id[TGT_LANG],\n            max_length=50\n        )\n        \n        # Decode the generated tokens into text\n        translated_text = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n        # print(translated_text)\n        translations.append(translated_text)\n    \n        writer.writerow({\n                    \"filename\": filename,\n                    \"id\": fileid,\n                    \"src\": src_text,\n                    \"tgt\": tgt_text,\n                    \"translated\": translated_text\n                })\n    \n    \n    # Print out translations for the entire dev set\n    for translated_text in translations:\n        print(translated_text)\n        break\n    \n    print(skip2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T19:08:53.981198Z","iopub.execute_input":"2025-05-04T19:08:53.981738Z","iopub.status.idle":"2025-05-04T19:10:12.205627Z","shell.execute_reply.started":"2025-05-04T19:08:53.981714Z","shell.execute_reply":"2025-05-04T19:10:12.204884Z"}},"outputs":[{"name":"stderr","text":"Processing test data: 100%|██████████| 113/113 [01:18<00:00,  1.44batch/s]","output_type":"stream"},{"name":"stdout","text":"ప్రధాన స్థానిక బీర్ ఒకటి. ఇది సంక్లిష్ట బీర్ కాదు కానీ సంతోషంగా మరియు రిఫ్రీట్ చేస్తారు. ఇతర స్థానిక బీర్ మన్టా అని పిలుస్తారు.\n0\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":71},{"cell_type":"code","source":"import pandas as pd\ndf= pd.read_csv(\"dev_asr_translations_speech_encoder.csv\")\nprint(df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T19:10:12.206727Z","iopub.execute_input":"2025-05-04T19:10:12.206982Z","iopub.status.idle":"2025-05-04T19:10:12.216513Z","shell.execute_reply.started":"2025-05-04T19:10:12.206966Z","shell.execute_reply":"2025-05-04T19:10:12.215800Z"}},"outputs":[{"name":"stdout","text":"                   filename    id  \\\n0  15158676295442294624.wav  1590   \n1  12952903060751652532.wav  1590   \n2  16131823300806444840.wav  1544   \n3  12470893547277455431.wav  1557   \n4  16359228487623086121.wav  1557   \n\n                                                 src  \\\n0  The main local beer is number one. It is not a...   \n1  The main local beer is number one. It is not a...   \n2  Insects were the first animals to take to the ...   \n3  After seeing the horrors and atrocities of war...   \n4  After seeing the horrors and atrocities of war...   \n\n                                                 tgt  \\\n0  \"ఇక్కడి ప్రధాన స్థానిక బీర్ 'Number One', ఇది ...   \n1  \"ఇక్కడి ప్రధాన స్థానిక బీర్ 'Number One', ఇది ...   \n2  కీటకాలు గాలిలోకి తీసుకువెళ్ళే మొదటి జంతువులు. ...   \n3  రెండవ ప్రపంచ యుద్ధసమయంలో జరిగిన ఘోరాలు, ఘోరాలు...   \n4  రెండవ ప్రపంచ యుద్ధసమయంలో జరిగిన ఘోరాలు, ఘోరాలు...   \n\n                                          translated  \n0  ప్రధాన స్థానిక బీర్ ఒకటి. ఇది సంక్లిష్ట బీర్ క...  \n1  ప్రధాన స్థానిక బీర్ ఒకటి. ఇది సంక్లిష్ట బీర్ క...  \n2  చీడలు గాలిలోకి తీసుకోవలసిన మొదటి జంతువులు. వార...  \n3  ప్రపంచ యుద్ధం సమయంలో యుద్ధం వేర్యలు మరియు దుర్...  \n4  ప్రపంచ యుద్ధం సమయంలో యుద్ధం వేర్యలు మరియు దుర్...  \n","output_type":"stream"}],"execution_count":72},{"cell_type":"code","source":"model.eval()\n\n# Define the source and target languages\nSRC_LANG = \"en_XX\"\nTGT_LANG = \"te_IN\"\n\n# Set the tokenizer source language\ntokenizer.src_lang = SRC_LANG\n\n# Placeholder for storing translations\ntranslations = []\n\n# Prepare CSV file (write header once)\noutput_csv = \"test_asr_translations_speech_encoder.csv\"\nwith open(output_csv, mode='w', newline='', encoding='utf-8') as f:\n    writer = csv.DictWriter(f, fieldnames=[\"filename\", \"id\", \"src\", \"tgt\", \"translated\"])\n    writer.writeheader()\n\n    skip2=0\n    # Iterate over the dev_loader\n    for batch in tqdm(test_asr_dataloader, desc=\"Processing test data\", unit=\"batch\"):\n        # Assuming 'input_ids' and 'attention_mask' are present in the batch\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        punct_feats = batch['punct_feats'].to(device)\n        \n        # Unwrap values from lists or tensors\n        filename = batch['filename'][0] if isinstance(batch['filename'], list) else batch['filename']\n        fileid = batch['id'].item() if isinstance(batch['id'], torch.Tensor) else batch['id']\n        src_text = batch['src'][0] if isinstance(batch['src'], list) else batch['src']\n        tgt_text = batch['tgt'][0] if isinstance(batch['tgt'], list) else batch['tgt']\n\n        \n        # Set the language token for the source\n        input_ids[0][0] = tokenizer.lang_code_to_id[SRC_LANG]\n        \n        # Run the encoder manually\n        with torch.no_grad():\n            encoder_outputs = model.model.encoder(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                return_dict=True\n            )\n    \n        try:\n            # Add punctuation features to encoder hidden states\n            combined_hidden_state = encoder_outputs.last_hidden_state\n        except:\n            skip2+=1\n            # print('skipping :(')\n            continue\n    \n        # Wrap into BaseModelOutput\n        encoder_outputs = BaseModelOutput(last_hidden_state=combined_hidden_state)\n    \n        # Generate translation from decoder with forced beginning token for Telugu\n        generated_tokens = model.generate(\n            input_ids=None,\n            encoder_outputs=encoder_outputs,\n            attention_mask=None,\n            forced_bos_token_id=tokenizer.lang_code_to_id[TGT_LANG],\n            max_length=50\n        )\n        \n        # Decode the generated tokens into text\n        translated_text = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n        # print(translated_text)\n        translations.append(translated_text)\n    \n        writer.writerow({\n                    \"filename\": filename,\n                    \"id\": fileid,\n                    \"src\": src_text,\n                    \"tgt\": tgt_text,\n                    \"translated\": translated_text\n                })\n    \n    \n    # Print out translations for the entire dev set\n    for translated_text in translations:\n        print(translated_text)\n        break\n    \n    print(skip2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T19:13:44.474753Z","iopub.execute_input":"2025-05-04T19:13:44.475456Z","iopub.status.idle":"2025-05-04T19:15:41.204763Z","shell.execute_reply.started":"2025-05-04T19:13:44.475430Z","shell.execute_reply":"2025-05-04T19:15:41.204074Z"}},"outputs":[{"name":"stderr","text":"Processing test data: 100%|██████████| 164/164 [01:56<00:00,  1.41batch/s]","output_type":"stream"},{"name":"stdout","text":"అదృష్టవశాత్తు, డ్రైవర్ ప్రవర్తనను 100 శాతం సురక్షితతో అంచనా వేయడం కష్టం.\n0\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":84},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}